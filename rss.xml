<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Frank Cozzolino - Blog &amp; Insights</title>
        <link>https://frankcozzolino.github.io/</link>
        <description>Thoughts, ideas, and professional insights on technology, project management, and professional development</description>
        <language>en-us</language>
        <lastBuildDate>Thu, 25 Sep 2025 21:20:54 GMT</lastBuildDate>
        <ttl>60</ttl>
        <image>
            <url>https://frankcozzolino.github.io/images/signature.png</url>
            <title>Frank Cozzolino</title>
            <link>https://frankcozzolino.github.io/</link>
        </image>
        
        <item>
            <title>The Lifelong Learning Manifesto: Why Education Must Be Reinvented for a 100+ Years of Working Life.</title>
            <link>https://frankcozzolino.github.io/blog.html?article=the-lifelong-learning-manifesto-why-education-must</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=the-lifelong-learning-manifesto-why-education-must</guid>
            <pubDate>Sun, 21 Sep 2025 09:11:10 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>education</category>
            <category>higher-education</category>
            <category>university</category>
            <category>philosophy</category>
            <description>How many times do you dare to begin again? There is not a fixed number, life is going to be a long long journey spawning centuries, do you really want to do the same job for 110 years in a row?We are...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*IJL_0QBwUAIDeJQJs7uM5Q.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IJL_0QBwUAIDeJQJs7uM5Q.png" /></figure><blockquote>How many times do you dare to begin again? There is not a fixed number, life is going to be a long long journey spawning centuries, do you really want to do the same job for 110 years in aÂ row?</blockquote><p>We are standing on the threshold of a revolution. Within 40 years from now the Advances in medicine, biotechnology, and technology will expand our lifespan indefinitely. A child born today may live not just 80 or 90 years, but 300 year. And they may not retire at 65. They might work for aÂ century.</p><p>This is not science fiction. It is the logical consequence of the world we are building. But our education systems, our cultural assumptions, and our social contracts remain frozen in the past. They are still built for a world where people study until 25, work until 65, and fade quietly into retirement.</p><p>That world isÂ gone.</p><h3>The End of the One-LifeÂ Model</h3><p>It is absurd to imagine a person doing one single job for 90 years. Humans are not machines built for repetitive output. We are explorers, creators, restless learners. Even the most rewarding career cannot stretch across a century without becoming aÂ prison.</p><p>An accountant should not have to remain an accountant until death. A doctor who dreams of becoming an engineer should not be told, â€œItâ€™s too late.â€ A scientist who feels called to agriculture should not face insurmountable barriers toÂ change.</p><p>In the coming era, people will live <em>multiple professional lives.</em> They will not have one career. They will have two, three, or four. And society must prepare forÂ that.</p><h3>The Failure of Todayâ€™s Education</h3><p>The current system is woefully inadequate. Adult education, as it exists today, is a patchwork of half-measures: short workshops, online certificates, â€œexecutive programs.â€ These are marketed as career boosters, rÃ©sumÃ© polishers, or incremental updates. But they are not designed to help a 50-year-old accountant become a doctor. They are not built to empower true reinvention.</p><p>Why? Because education today is treated as a <strong>business model</strong>, not as a <strong>social necessity.</strong> Institutions sell knowledge like a commodity. They focus on young students because thatâ€™s the â€œprofitableâ€ market. Adults are left with scraps light trainings that change nothing fundamental.</p><p>This is notÂ enough.</p><h3>The Vision: Education as a LifelongÂ Right</h3><p>We must re imagine education as an engine of lifelong transformation. Not a one-time rite of passage, but a system that travels with you through every decade ofÂ life.</p><p>This means:</p><ul><li><strong>Universal access to midlife education.</strong> Just as children are guaranteed primary schooling, adults must be guaranteed the right to re-educate themselves at 40, 60, orÂ 80.</li><li><strong>Full immersion retraining.</strong> Not token courses, but 3â€“5 years of deep study that allow for real careerÂ shifts.</li><li><strong>Financial and societal support.</strong> Just as societies support parental leave, we must support <em>educational leave.</em> People must be able to step away from work without falling intoÂ poverty.</li><li><strong>Cultural redefinition of success.</strong> Career changes must be celebrated as acts of courage and renewal, not treated as failures or signs of instability.</li><li><strong>Policy reform.</strong> Governments must see lifelong education as infrastructure, not as a privateÂ luxury.</li></ul><h3>Living Many Lives WithinÂ One</h3><p>The greatest promise of extended life is not simply more years. It is more <em>lives.</em> Each transformation enriches not only the individual but society as a whole. Experience compounds. Wisdom travels across fields. Innovation flourishes when a doctor thinks like an engineer, or a farmer thinks like a scientist.</p><h3>A message from ourÂ Founder</h3><p><strong>Hey, </strong><a href="https://linkedin.com/in/sunilsandhu"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We donâ€™t receive any funding, we do this to support the community. â¤ï¸</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href="https://linkedin.com/in/sunilsandhu"><strong>LinkedIn</strong></a><strong>, </strong><a href="https://tiktok.com/@messyfounder"><strong>TikTok</strong></a>, <a href="https://instagram.com/sunilsandhu"><strong>Instagram</strong></a>. You can also subscribe to our <a href="https://newsletter.plainenglish.io/"><strong>weekly newsletter</strong></a>.</p><p>And before you go, donâ€™t forget to <strong>clap</strong> and <strong>follow</strong> theÂ writerï¸!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=31ee784d11ae" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/the-lifelong-learning-manifesto-why-education-must-be-reinvented-for-a-100-years-of-working-life-31ee784d11ae">The Lifelong Learning Manifesto: Why Education Must Be Reinvented for a 100+ Years of Working Life.</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>Do Mushrooms Communicate? A Concise, Scientific Look at Fungal Signalling</title>
            <link>https://frankcozzolino.github.io/blog.html?article=do-mushrooms-communicate-a-concise-scientific-look</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=do-mushrooms-communicate-a-concise-scientific-look</guid>
            <pubDate>Sat, 16 Aug 2025 16:24:50 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>biology</category>
            <category>curiosity</category>
            <category>mushrooms</category>
            <category>communication</category>
            <category>science</category>
            <description>Fungi do not have nerves or brains. Yet their tissues conduct electricity. Recent work suggests these signals are structured, not random. Some patterns even mimic statistics seen in human language....</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*tE7vLDQArpkZOzBIc4CbeQ.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tE7vLDQArpkZOzBIc4CbeQ.png" /></figure><p>Fungi do not have nerves or brains. Yet their tissues conduct electricity. Recent work suggests these signals are structured, not random. Some patterns even mimic statistics seen in human language. The claim is bold. The data deserve a clear, soberÂ read.</p><h4>The Substrate: Mycelium as a Signalling Network</h4><p>Beneath fruiting bodies lies <strong>mycelium</strong>â€Šâ€”â€Šanastomosing threads (hyphae) that infiltrate soil, wood, and roots. This network moves water, ions, and metabolites. It also exhibits <strong>spiking electrical activity</strong>. Think of it less as speech and more as <strong>distributed telemetry</strong> within a livingÂ lattice.</p><h4>What WasÂ Measured</h4><p>Adamatzky and colleagues inserted microelectrodes into lab-grown cultures of four species: <strong>Flammulina velutipes</strong> (enoki), <strong>Schizophyllum commune</strong> (split gill), <strong>Omphalotus nidiformis</strong> (ghost), and <strong>Cordyceps militaris</strong> (caterpillar fungus). They recorded <strong>spike trains</strong> for days. They then parsed the trains into clusters using simple rules. The clusters behaved like <strong>putativeÂ â€œwords.â€</strong></p><h4>Languageâ€‘Like Structure (WithÂ Caveats)</h4><p>Analyses suggested a <strong>lexicon</strong> of ~50 distinct clusters. Only <strong>15â€“20</strong> occurred often. The <strong>mean â€œwordâ€ length</strong> was ~6 symbols, close to values seen in human texts. Signal rates rose after <strong>perturbations</strong> such as damage or nutrient contact, implying <strong>context-sensitive modulation</strong>. These are <strong>statistical</strong> parallels, not proof of semantics.</p><h4>Interpretation</h4><p>Do fungi â€œtalkâ€? Probably not in any human sense. The spikes might reflect <strong>ion fluxes, growth fronts, stress responses</strong>, or <strong>resource allocation dynamics</strong>. Calling clusters â€œwordsâ€ risks <strong>anthropomorphism</strong>. A cautious phrasing is better: <strong>mycelial spike trains show compressible, structured dynamics</strong> that <strong>encode</strong> internal and environmental state. The <strong>semantics remainÂ opaque</strong>.</p><h4>Why ItÂ Matters</h4><p>Even without semantics, structure implies <strong>information processing</strong>. That mattersÂ for:</p><ul><li><strong>Ecophysiology:</strong> non-chemical signalling that coordinates growth andÂ repair.</li><li><strong>Biosensing:</strong> living materials that report <strong>substrate status</strong> with lowÂ power.</li><li><strong>Neuromorphic and biomimetic computing:</strong> <strong>lowâ€‘energy, eventâ€‘driven</strong> architectures inspired by hyphal spikeÂ trains.</li></ul><h4>What to TestÂ Next</h4><p>Progress needs rigor, notÂ hype:</p><ul><li><strong>Field telemetry</strong> in intact soils, not only petriÂ dishes.</li><li><strong>Perturbation assays</strong> with preregistered hypotheses (wounding, nutrients, competitors).</li><li><strong>Decoding attempts</strong> (state-space models, information-theoretic metrics) to link signals to <strong>ground-truth states</strong>.</li><li><strong>Cross-kingdom effects:</strong> whether plant behavior shifts when <strong>fungal signals</strong>Â change.</li><li><strong>Stimulus specificity:</strong> responses to light, vibration, or sound, with controls.</li></ul><h4>Bottom Line</h4><p>Fungal networks generate <strong>structured, stimulusâ€‘responsive electrical activity</strong>. The patterns resemble language statistics but likely arise from <strong>physiology, not conversation</strong>. Still, the system is fascinating: <strong>distributed, frugal, and robust</strong>. Understanding it could reframe how we see communicationâ€Šâ€”â€Šand computationâ€Šâ€”â€Šin livingÂ matter.</p><h3>References</h3><ul><li>Adamatzky, A. (2022). <em>Language of fungi derived from electrical spiking activity.</em> Royal Society Open Science. <a href="https://arxiv.org/abs/2112.09907">DOI link / ArXivÂ preprint</a></li><li>The Guardianâ€Šâ€”â€Š<em>Fungi generate electrical impulses similar to human speech, study finds</em>Â (<a href="https://www.theguardian.com/science/2022/apr/06/fungi-electrical-impulses-human-language-study?utm_source=chatgpt.com">link</a>)</li><li>Smithsonian Magazineâ€Šâ€”â€Š<em>Mushrooms May Communicate With Each Other Using Electrical Impulses</em>Â (<a href="https://www.smithsonianmag.com/smart-news/mushrooms-may-communicate-with-each-other-using-electrical-impulses-180979889/?utm_source=chatgpt.com">link</a>)</li><li>IFLScienceâ€Šâ€”â€Š<em>Mushrooms May Talk To Each Other And Have Vocabulary Of 50 Words</em>Â (<a href="https://www.iflscience.com/mushrooms-may-talk-to-each-other-and-have-vocabulary-of-50-words-63236?utm_source=chatgpt.com">link</a>)</li><li>CBS Newsâ€Šâ€”â€Š<em>Study suggests mushrooms may talk to each other</em>Â (<a href="https://www.cbsnews.com/texas/news/study-suggests-mushrooms-may-talk-to-each-other/?utm_source=chatgpt.com">link</a>)</li><li>Discover Magazineâ€Šâ€”â€Š<em>If Fungi Could Talk: Study Suggests Fungi Could Communicate in Structured Ways</em>Â (<a href="https://www.discovermagazine.com/if-fungi-could-talk-study-suggests-fungi-could-communicate-in-structure-43541?utm_source=chatgpt.com">link</a>)</li><li>Altalangâ€Šâ€”â€Š<em>Mushrooms Communicate</em> (<a href="https://altalang.com/beyond-words/mushrooms-communicate/?utm_source=chatgpt.com">link</a>)</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e9cae5f918ef" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>â€œAI isnâ€™t delivering valueâ€ and â€œWhy Firing People for AI Agents is a Costly Mistakeâ€â€Šâ€”â€Š40% ofâ€¦</title>
            <link>https://frankcozzolino.github.io/blog.html?article=ai-isnt-delivering-value-and-why-firing-people-for</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=ai-isnt-delivering-value-and-why-firing-people-for</guid>
            <pubDate>Thu, 17 Jul 2025 08:20:20 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>budget</category>
            <category>product-management</category>
            <category>business-strategy</category>
            <category>corporate-culture</category>
            <category>ai</category>
            <description>â€œAI isnâ€™t delivering valueâ€ and â€œWhy Firing People for AI Agents is a Costly Mistakeâ€ â€” 40% of Agentic AI projects will fail.when strategy, measurement, and execution are weak and over 40% of Agentic...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*-XloYdAw_hSW_izDAL06_Q.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3>â€œAI isnâ€™t delivering valueâ€ and â€œWhy Firing People for AI Agents is a Costly Mistakeâ€â€Šâ€”â€Š40% of Agentic AI projects willÂ fail.</h3><blockquote>when strategy, measurement, and execution are weak and over 40% of Agentic AI projects will fail by the end of 2027 due to cost hikes, murky ROI, and integration issues.</blockquote><blockquote>68% of companies are spending $50M to $250M annually on GenAIâ€Šâ€”â€Šonly 31% expect to measure ROI within sixÂ months</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-XloYdAw_hSW_izDAL06_Q.png" /></figure><p><strong>Introduction</strong></p><p>Enterprises worldwide are embracing generative AI (GenAI) with unmatched enthusiasm. Yet behind the optimism lies a cautionary tale: <strong>rushing into AI adoption without strategic planning, financial oversight, and a clear purpose can lead to budget overruns, failed projects, and a hollow return on investment (ROI).</strong> Perhaps most dangerously, some firms are laying off staff prematurely, betting on AI agents to replace human capital. But as the evidence shows, <strong>this short-sighted strategy often leads to higher costs, compliance risks, and operational inefficiencies.</strong></p><p><strong>1. Ballooning Budgets and UnseenÂ Costs</strong></p><p>Gartner forecasts GenAI-related spending to top <strong>$644 billion in 2025</strong>, with many enterprises surprised by costs not accounted for upfront. As seen in Thermo Fisher Scientificâ€™s chatbot pilot, consumption-based pricing quickly spiraled out of control. What began as a promising automation initiative nearly collapsed due to unpredictable data usage fees and model hallucinations that posed compliance issues (<a href="https://www.computerworld.com/article/4021954/rushing-into-genai-prepare-for-budget-blowouts-and-broken-promises.html">Computerworld</a>).</p><p>Capgemini CEO Aiman Ezzat highlighted another failure: an internal chatbot was projected to cost <strong>$25 million annually</strong> in data processingâ€Šâ€”â€Šforcing its termination before launch. The lesson: GenAI tools often <strong>require more compute, integration, and oversight</strong> than anticipated, and costs compound rapidly withÂ scale.</p><p>According to Gartner, custom GenAI models can cost <strong>$5â€“20 million to build</strong>, plus <strong>$8,000â€“21,000 per user per year</strong>, with API usage adding hundreds of thousands more. These expenses often surprise companies who budget only for initial implementation.</p><p><strong>2. ROI RemainsÂ Elusive</strong></p><p>Despite massive investmentsâ€Šâ€”â€Š<strong>68% of companies are spending $50M to $250M annually</strong> on GenAIâ€Šâ€”â€Šonly <strong>31% expect to measure ROI within six months</strong> (<a href="https://www.cio.com/article/3778320/enterprises-willing-to-spend-up-to-250-million-on-gen-ai-but-roi-remains-elusive.html">CIO.com</a>). Organizations fail to align AI projects with core KPIs, and many ignore foundational needs like data governance, model explainability, and human oversight.</p><p>Even in analytics, where AI and automation should shine, companies struggle to quantify value. A TechRadar report found success only when IT leaders define metrics upfront, invest in training, and connect automation outcomes with businessÂ goals.</p><p>A McKinsey/CIO study confirms that just <strong>31% anticipate measurable value shortly</strong>, citing data, governance, and alignment as consistent lags. The lesson is clear: without discipline and alignment, even generous GenAI budgets fail toÂ deliver.</p><p><strong>3. Project Failures and the AI HypeÂ Hangover</strong></p><p>Gartner analysts say GenAI is moving past its â€œpeak hypeâ€ into the <strong>â€œtrough of disillusionment.â€</strong> Many deployments in 2024 were scrapped due to unclear value or misaligned expectations (<a href="https://www.itpro.com/business/business-strategy/generative-ai-enthusiasm-continues-to-beat-out-business-uncertainty">ITPro</a>). Nearly <strong>42% of AI initiatives</strong> fail before reaching production.</p><p>Reasons include:</p><ul><li>Poor integration with legacyÂ systems</li><li>Inaccurate or low-quality data</li><li>Ethical/legal blindÂ spots</li><li>Lack of cross-functional leadership</li></ul><p>These pitfalls are exacerbated when firms remove experienced staff who would otherwise provide the institutional knowledge and governance required forÂ success.</p><p>The Economist reports that abandonment of AI pilots leapt from 17% to <strong>42%</strong> in one year, with some companies rehiring staff to fix flawed systems. Anthropicâ€™s vending machine AI failed in live trialsâ€Šâ€”â€Šmispricing goods and generating nonsenseâ€Šâ€”â€Šproving that human-in-the-loop oversight is not optional.</p><p><strong>4. The Human Cost: Layoffs and False Economies</strong></p><p>Many companies now equate productivity with profitability and mistakenly see GenAI as a labor cost-cutting tool. But firing employees to replace them with AI agents creates <strong>false economies.</strong></p><p>AI agents stillÂ require:</p><ul><li>Manual validation and oversight</li><li>Regular prompt engineering andÂ tuning</li><li>Ongoing retraining on updatedÂ datasets</li></ul><p>Scale AI laid off 14% of its workforce after its GenAI pods grew too expensive and underperformed. Klarna replaced 700 service jobs with AI but quickly reversed course when quality and satisfaction dropped.</p><p>As Capgeminiâ€™s CEO warned, <strong>productivity doesnâ€™t equal savings</strong>, especially in lower-wage roles. And as RHR Internationalâ€™s CIO noted, successful projects budget for human governance and experimentationâ€Šâ€”â€Šfactors impossible to replace entirely withÂ AI.</p><p>A Stanford study found that hybrid human-AI teams are <strong>14% more productive</strong> and deliver higher satisfaction than either alone. AI deployment without integrated human roles leads to chaos, not efficiency.</p><p><strong>Conclusion: Invest in AI <em>with</em> People, Not <em>instead</em> ofÂ Them</strong></p><p>Generative AI holds enormous potential. But chasing savings by cutting headcount and offloading critical roles to AI agents is a misstep that will likely cost companies more in the long runâ€Šâ€”â€Šthrough failed projects, regulatory risks, and lostÂ trust.</p><p><strong>AI should augment, not replace, your workforce.</strong></p><p>The future belongs to those who pair machine intelligence with human judgmentâ€Šâ€”â€Šstrategically, ethically, and with financial clarity. Companies that rush blindly forward will pay twice: once in money, and again in lost opportunity.</p><h3>Thank you for being a part of the community</h3><p><em>Before youÂ go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writerÂ ï¸ğŸ‘<strong>ï¸ï¸</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> |Â <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a>Â ğŸš€</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a>Â ğŸ§‘ğŸ»â€ğŸ’»</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=03b77684a261" width="1" height="1" alt=""><hr><p><a href="https://blog.venturemagazine.net/ai-isnt-delivering-value-and-why-firing-people-for-ai-agents-is-a-costly-mistake-40-of-03b77684a261">â€œAI isnâ€™t delivering valueâ€ and â€œWhy Firing People for AI Agents is a Costly Mistakeâ€â€Šâ€”â€Š40% ofâ€¦</a> was originally published in <a href="https://blog.venturemagazine.net">Venture</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>I am Sorry, but We will never talk to ET</title>
            <link>https://frankcozzolino.github.io/blog.html?article=i-am-sorry-but-we-will-never-talk-to-et</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=i-am-sorry-but-we-will-never-talk-to-et</guid>
            <pubDate>Thu, 26 Jun 2025 15:35:01 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>astronomy</category>
            <category>astrophysics</category>
            <category>ufology</category>
            <category>ufo</category>
            <category>ufos-and-aliens</category>
            <description>The life in the universe might not be rare, but definitely rarer than what we expected.The Drake equation has a lot of guessing, but even in our Solar system and nearby stars we tend to be alone....</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*59rASJy1onpWv2dsBBhPrA.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*59rASJy1onpWv2dsBBhPrA.png" /></figure><p>The life in the universe might not be rare, but definitely rarer than what we expected.</p><p>The Drake equation has a lot of guessing, but even in our Solar system and nearby stars we tend to be alone. Even microbes which should be everywhere seems to not be present anywhere in our Solar System (so far), which should be a good indicator that even the parameters below might be not very conservatives.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/928/1*ZBKoQsog-Lmakh3Stc0mWQ.png" /></figure><p>50000 species in the entire universe, since the beginning. Seems extremely small amount, and it is frightening to think about the implication of thatÂ number.</p><p>Nonetheless, we have another obstacle, thatâ€™s the great filter, and our technological revolution is extremely young. Moreover, we arrived very late to the party. Most of species could be already be extincted billions of yearsÂ ago.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/791/1*hzTXCYTx9gwrDiymxCSl4Q.png" /><figcaption>We ourselves appear at <strong>13.8 Gyr â†’ in the â€œLateâ€ epoch</strong>, near the trailing edge of cosmic habitability.</figcaption></figure><p>So, accounting for this the probability that we detect any other specie is basically 0.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/810/1*rzw6RsPsGo_s4nf38zXTAQ.png" /><figcaption>table showing, for each scenario, the expected number of detectable civilizations within our 100 ly listening volume and the corresponding probability (assuming a Poisson process, so P(â‰¥1)â‰ˆNdetectP(\ge1)\approx N_{\rm detect}P(â‰¥1)â‰ˆNdetectâ€‹ for Ndetectâ‰ª1N_{\rm detect}\ll1Ndetectâ€‹â‰ª1):</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=358236cdb664" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>Little-known Google Auth Feature that allows you to exchange PingFederate Token with Google Accessâ€¦</title>
            <link>https://frankcozzolino.github.io/blog.html?article=littleknown-google-auth-feature-that-allows-you-to</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=littleknown-google-auth-feature-that-allows-you-to</guid>
            <pubDate>Thu, 26 Jun 2025 08:23:16 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>google</category>
            <category>token</category>
            <category>authorization</category>
            <category>pingfederate</category>
            <category>google-cloud-platform</category>
            <description>Little-known Google Auth Feature that allows you to exchange PingFederate Token with Google Access TokenğŸ« The Golden Ticket (Authorization Code)As a golden ticket in the OAuth 2.0 flow, the...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/0*Ii2CrjYQfJw0L0Ud" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3>Little-known Google Auth Feature that allows you to exchange PingFederate Token with Google AccessÂ Token</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ii2CrjYQfJw0L0Ud" /></figure><h3>ğŸ« The Golden Ticket (Authorization Code)</h3><p>As a golden ticket in the OAuth 2.0 flow, the authorization code is a precious, short-lived credential representing proof of user consent. Issued by Googleâ€™s authorization server once a user authenticates and grants your application the requested scopes, this single-use code is an 80â€Šâ€”â€Š120-character, Base64-encoded string (e.g., â€œ4/0AVG7fiQ7Gâ€¦â€) that is bound to your appâ€™s client_id and the exact redirect_uri used in the initial request. Though it travels through potentially insecure channels like browser URL parameters, it remains useless to any attacker because it can only be redeemed at Googleâ€™s token endpoint in exchange for access and refresh tokens when presented alongside your client_secret. With a strict expiration window of about ten minutes and built-in cryptographic safeguards, the authorization code can safely bridge user consent to long-term API credentials without exposing your appâ€™sÂ secrets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2EFok-VbgwEMEOlL" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pv0Pkf4KyTmcBmC5" /></figure><h3>Example</h3><pre>const tokenResponse = await fetch(&#39;https://oauth2.googleapis.com/token&#39;, {<br>    method: &#39;POST&#39;,<br>    headers: { &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39; },<br>    body: new URLSearchParams({<br>      code,<br>      client_id: process.env.GOOGLE_CLIENT_ID || &#39;&#39;,<br>      client_secret: process.env.GOOGLE_CLIENT_SECRET || &#39;&#39;,<br>      redirect_uri: process.env.GOOGLE_REDIRECT_URI || &#39;&#39;,<br>      grant_type: &#39;authorization_code&#39;,<br>    }),<br>  });</pre><h3>References</h3><p><a href="https://developers.google.com/identity/protocols/oauth2">https://developers.google.com/identity/protocols/oauth2</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=679573ade04f" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>The WWW is almost dead, what next?</title>
            <link>https://frankcozzolino.github.io/blog.html?article=the-www-is-almost-dead-what-next</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=the-www-is-almost-dead-what-next</guid>
            <pubDate>Wed, 25 Jun 2025 09:19:23 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>democracy</category>
            <category>ai</category>
            <category>humanity</category>
            <category>social-media</category>
            <category>internet</category>
            <description>From the gritty garages of Silicon Valley to the neon lit high rises of Manhattan, the World Wide Web burst onto the scene in 1991 as Tim Berners Leeâ€™s brainchild at CERN. What started as a simple...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*fkaVio8nZNIIybBB6Xzmeg.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fkaVio8nZNIIybBB6Xzmeg.png" /></figure><p>From the gritty garages of Silicon Valley to the neon lit high rises of Manhattan, the World Wide Web burst onto the scene in 1991 as Tim Berners Leeâ€™s brainchild at CERN. What started as a simple network of hyperlinked documents quickly morphed into a digital coliseum, with browsers duking it out on desktop stages and dot com fortunes rising and crashing faster than a subway express. By the turn of the millennium, the Web had rewired everything newsstands, corner bodegas, Wall Street tickers turning every New Yorker into a pixel in the globalÂ grid.</p><p>In the 1990s, the Internet kicked off as a noisy neon bazaar: dialâ€‘up modems chirped in suburban bedrooms while Netscape Navigator ruled the streets, and Geocities homepages bloomed like wild graffiti. Bulletin boards and IRC channels became the cool underground clubs where chat room denizens swapped ASCII art and MP3s on Napster lit the fire for digital file sharing. By the turn of the millennium, the dotâ€‘com boom turned startups into overnight Wall Street darlings pets.com and pets spelled out the risks of unbridled hype when the bubble burst in 2000.<br>The early 2000s reloaded the Web into Version 2.0: blogs sprouted like newsstands on every corner, MySpaceâ€™s rocker profiles and Friendsterâ€™s exclusive clubs set the template, and eâ€‘commerce giants like Amazon and eBay turned oneâ€‘click buying into a citywide habit. Streaming video and podcasts began rumbling beneath the surface, while broadband quietly replaced dialâ€‘up, ushering in faster scrolls and richer media. This era proved that between the dialâ€‘up thunks and the postâ€‘bubble revelations, the Internet wasnâ€™t just a passing fad but a gritty, evolving metropolis one built on community, commerce, and the code to keep hustling.</p><p>Enter the AI era, the World Wide Web, once a sprawling agora of shared ideas and unbridled curiosity, lies comatose. In its place stands an ironclad ecosystem of â€œknowledgeâ€ meticulously distilled into soulless shards by faceless corporations and opaque government bureaus. Welcome to our new digital age: a sterile, privatized vault where every byte of information is bartered, fenced off, and rationed to suit the whims of remote dataÂ barons.</p><p>Digital isolation has become the default. No longer do communities gather in bustling chatrooms or exchange ideas freely across blogs. Instead, individuals retreat into algorithmically sanctioned bubbles small, well lit cages that hum with the comforting glow of curated content. Donâ€™t bother wandering beyond the paywalls and firewalls; youâ€™ll find nothing but echoes of your own preferences, an intoxicating loop designed to keep you clicking and consuming until the model dictates otherwise.<br>Knowledge fenced gardens are flourishing. Major tech conglomerates now host â€œpremiumâ€ archives of the human record, available only via subscription tiers that rival luxury car leases. Want to read the complete works of Shakespeare? Thatâ€™ll be â‚¬19.99 a month, please. Need the latest climate data? First you must verify your identity and prove youâ€™ve purchased the â€œEnterprise Insightsâ€ package. These walled off libraries masquerade as convenience, but the truth is blatant: profit has trumped public good, and weâ€™re left paying tribute to invisible gatekeepers.<br>Privatization of knowledge isnâ€™t subtle. Governments, under the guise of â€œnational security,â€ have signed lucrative licensing deals with private model providers, effectively offloading public archives into corporate vaults. What was once funded by tax dollars is now repackaged as exclusive â€œinsightsâ€ for a privileged few. Even academic research once a bastion of open peer review has been siphoned into restricted APIs that throttle access unless you hold the right credentials or creditÂ card.</p><p>The social fabric has frayed. We are less social, more alone in our digital cocoons. Physical proximity no longer guarantees authenticity; nor does online proximity. Your next door neighbor might be a manufactured persona in a mass produced chat interface. Friends you thought you knew on Discord could just as easily be bots, churning out canned sympathies in response to your darkest confessions. Emotional labor is outsourced to simulacra, making genuine human connection a relic of a bygone era.<br>Once the go to hunting grounds for late night confidences and pixel perfect romances chatrooms, forums and dating apps now reek of synthetic sincerity. You pour your heart out over text or even video, only to realize the â€œpersonâ€ on the other end might be nothing more than a neural puppet. Those laugh track jokes, the perfectly timed empathies, the coy good morning selfies theyâ€™re all expertly generated by black box models designed to keep you hooked. You canâ€™t swipe right on authenticity when every profile could be a cleverly animated faÃ§ade, every â€œI miss youâ€ a line of code, and every â€œletâ€™s video chatâ€ a deepfake waiting to expose your most private moments to corporate auditors. And hereâ€™s the clincher: you never know whether youâ€™re talking to a human or a neural network. Every article, every comment, every â€œliveâ€ video stream is suspect. Content could be fabricated from scratch by generative models that have ingested our collective culture and spat it back out, polished to a deceptive shine. Skepticism is the only currency left yet skepticism itself has been commodified into â€œfact checkingâ€ services that charge perÂ claim.</p><p>So, farewell to the democratized dream of the early internet. The web is dead, and all hail the new overlords ofÂ data.</p><p>To be continuedÂ â€¦</p><h3>Thank you for being a part of the community</h3><p><em>Before youÂ go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writerÂ ï¸ğŸ‘<strong>ï¸ï¸</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> |Â <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a>Â ğŸš€</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a>Â ğŸ§‘ğŸ»â€ğŸ’»</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a1199883f200" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/the-www-is-almost-dead-what-next-a1199883f200">The WWW is almost dead, what next?</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>Vibe Coding 2/5 â€”From Vibe coding to UML Coding.</title>
            <link>https://frankcozzolino.github.io/blog.html?article=vibe-coding-25-from-vibe-coding-to-uml-coding</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=vibe-coding-25-from-vibe-coding-to-uml-coding</guid>
            <pubDate>Mon, 23 Jun 2025 07:50:06 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>cursor-ai</category>
            <category>vibe-coding</category>
            <category>cursor</category>
            <category>uml-diagrams</category>
            <category>ai-coding</category>
            <description>â€œVibe coding is fun, but it lacks professionalism and causes many people to burn out due to bad results. Luckily, CS classes have taught UML design for pretty much forever. Almost no vibe coders...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*pcUG-skiFHVpVHCPO1lbRg.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<p>â€œVibe coding is fun, but it lacks professionalism and causes many people to burn out due to bad results. Luckily, CS classes have taught UML design for pretty much forever. Almost no vibe coders today use UML; most merely â€œtype with the model.â€ No wonder they feel theÂ drain.â€</p><p>Read â€œVibe Coding 2/5â€Šâ€”â€ŠFrom Vibe coding to UML Coding â€œ by Francesco C. on Medium: <a href="https://medium.com/@francesco.cozzolino/vibe-coding-2-5-from-vibe-coding-to-uml-coding-711e5510de29">https://medium.com/@francesco.cozzolino/vibe-coding-2-5-from-vibe-coding-to-uml-coding-711e5510de29</a></p><h3>The book</h3><p>I strongly advice everyone to purchase this book and absorb everything said. This book pretty much changed my life and made me better, showed me the way, this goes beyond programming, this methodology you can apply to any Big Problem you will face in theÂ life.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pcUG-skiFHVpVHCPO1lbRg.png" /></figure><h3>The Shift from Coders to Architects</h3><p>Think of it as an evolution, not a replacement. The arrival of AI in software development isnâ€™t about eliminating our rolesâ€Šâ€”â€Šitâ€™s about transforming them. Instead of spending 99% of our time typing lines of code, weâ€™ll shift our focusÂ toward:</p><ul><li><strong>Architectural designÂ (UML)</strong></li><li><strong>Security protocols</strong></li><li><strong>AI strategy and governance</strong></li></ul><p>In effect, we become <strong>project architects by default</strong>. Our deep knowledge of design patterns, threat modeling, and system diagrams becomes the currency of innovation, while AI handles the boilerplate.</p><h3>The Big-Picture Imperative</h3><p>Itâ€™s like when hunter-gatherers first turned to agriculture. They no longer spent every waking moment foraging; they could build sturdier homes, sharpen better tools, and cultivate culture. If our goal is to conquer the galaxy, we canâ€™t do it one line of code at a time. We need to understand the architecture of the universeÂ itself.</p><p>AI liberates us from the minutiae: the endless debugging, the repetitive refactoring, the routine unit tests. Instead, we ask bigger questions:</p><ul><li>How do we secure quantum-resistant communication?</li><li>What standards govern AI ethics across interplanetary networks?</li><li>How can we design self-healing architectures that adapt in realÂ time?</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nGx1xUK18QbaGlw0-k_Jrw.png" /></figure><h3>What isÂ UML?</h3><p>UML Unified Modeling Language (UML) is a powerful, standardized modeling language that emerged from the field of software engineering, primarily designed to specify, visualize, construct, and document software systems. Developed in the 1990s by industry leaders Grady Booch, James Rumbaugh, and Ivar Jacobson, UML became a critical tool for software developers, providing clarity and consistency in system design and facilitating effective communication among development teams. Its broad acceptance is due largely to its flexibility, scalability, and ability to manage complexity through visual abstraction and standardized notation. At its core, UML provides a structured visual language with multiple diagram types that address different aspects of system design and analysis. These diagram types include structural diagrams such as class diagrams and object diagrams, as well as behavioral diagrams including use-case diagrams, activity diagrams, and sequence diagrams. Each diagram type is tailored to effectively represent different perspectives of systemsâ€Šâ€”ï¿½ï¿½whether static relationships, interactive behaviors, or dynamic sequences of operations. In the context of analyzing scientific papers, UMLâ€™s robust ability to represent complex interactions and logical sequences becomes particularly valuable. Sequence diagrams, one of UMLâ€™s behavioral diagram types, are especially suited to illustrating dynamic interactions and the temporal progression of events. They clearly depict how different entities or components communicate and interact over time, making them ideal for modeling logical arguments, evidence flows, and conclusions within academic literature.</p><h3>The main 3 diagrams you need toÂ master</h3><h3>Use CaseÂ Diagram</h3><p>Purpose: Captures functional requirements by showing the interactions between external actors (users or systems) and theÂ feature.</p><p>When to use: At the very start of feature design, to align stakeholders on what the feature must do and who will useÂ it.</p><h4>Key elements:</h4><p>Actors (stick figures): represent roles interacting with theÂ system</p><p>Use cases (ovals): represent specific user goals or functions</p><p>System boundary (rectangle): encapsulates the featureâ€™s scope</p><p>Associations (lines): connect actors to the use cases they participate in</p><p>Benefits: Clarifies scope, highlights user needs, uncovers missing requirements.</p><h3>Sequence Diagram</h3><p>Purpose: Models the dynamic interactions and message flows between objects/components overÂ time.</p><p>When to use: Once you know the key actors and use cases; to detail the step-by-step flow for a particular scenario or operation within theÂ feature.</p><h4>Key elements:</h4><p>Lifelines (vertical dashed lines): represent participant objects or components</p><p>Activation bars (thin rectangles on lifelines): indicate when an object isÂ active</p><p>Messages (horizontal arrows): show calls or data transfers, labeled with operation names</p><p>Return messages (dashed arrows): show responses or returnedÂ values</p><p>Benefits: Identifies required interfaces, clarifies sequencing and concurrency, surfaces timing or ordering issuesÂ early.</p><h3>Class Diagram</h3><p>Purpose: Defines the static structure of the feature by showing classes, their attributes, methods, and relationships.</p><p>When to use: After youâ€™ve specified interactions (via sequence diagrams) and know which objects must exist; to consolidate data structures and API contracts.</p><h4>Key elements:</h4><p>Classes (rectangles divided into three compartments): name, attributes, and operations</p><p>Associations (lines): depict relationships (e.g., one-to-many) betweenÂ classes</p><p>Inheritance/generalization (lines with hollow triangle): show parent/child hierarchies</p><p>Dependencies (dashed arrows): indicate usage without ownership</p><p>Benefits: Provides a clear blueprint for implementation, aids in code generation or manual coding, ensures a consistent objectÂ model.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=711e5510de29" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>Superintelligence Isnâ€™t Imminentâ€Šâ€”â€ŠAnd The Illusion of Thinking.</title>
            <link>https://frankcozzolino.github.io/blog.html?article=superintelligence-isnt-imminent-and-the-illusion-o</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=superintelligence-isnt-imminent-and-the-illusion-o</guid>
            <pubDate>Tue, 17 Jun 2025 08:57:14 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>artificial-intelligence</category>
            <category>ai-research</category>
            <category>ai</category>
            <category>cognitive-science</category>
            <category>machine-learning</category>
            <description>ğŸ§  Superintelligence Isnâ€™t Imminent â€” And The Illusion of Thinking.Despite all the hype around superintelligent AI systems poised to surpass human capabilities, a fresh wave of research paints a very...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*ZshpwrA1mv_wEd7aAhvlgA.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3>ğŸ§  Superintelligence Isnâ€™t Imminentâ€Šâ€”â€ŠAnd The Illusion of Thinking.</h3><p>Despite all the hype around superintelligent AI systems poised to surpass human capabilities, a fresh wave of research paints a very differentâ€Šâ€”â€Šand much more groundedâ€Šâ€”â€Špicture. Appleâ€™s recent study, <em>â€œThe Illusion of Thinkingâ€</em>, alongside contributions from leading researchers and media outlets, calls into question the narrative of imminent Artificial General Intelligence (AGI). What weâ€™re witnessing, it seems, isnâ€™t the rise of a digital Einsteinâ€Šâ€”â€Šbut something more brittle, narrow, and surprisingly easy toÂ fool.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZshpwrA1mv_wEd7aAhvlgA.png" /></figure><h3>1. Recent Findings Challenge theÂ Hype</h3><p>In <em>â€œThe Illusion of Thinkingâ€</em>, Apple tested leading large language models (LLMs) from OpenAI, Anthropic, and Google on classic reasoning puzzles like Tower of Hanoi and River Crossing. The results were startling: the models, often perceived as near-omniscient, failed logic problems solvable by school children. Worse, as task complexity increased, their ability to complete the problems dropped sharplyâ€Šâ€”â€Šsometimes they even stopped trying mid-task.</p><h4>Recent Findings Challenge the AIÂ Hype</h4><p>Appleâ€™s new 2025 study <em>â€œThe Illusion of Thinkingâ€</em> put leading chain-of-thought LLMs to the test with classic logic puzzles. The team evaluated models from OpenAI (o3), Anthropic (Claude 3.7), and Google (Gemini) on tasks like Tower of Hanoi, a river-crossing problem, checkers jumping, and block-stacking. These puzzles were scaled from trivial (e.g. 1-disk Hanoi) to extremely hard (e.g. 20-disk Hanoi requiring over a million moves). In practice, these are problems a schoolchild can solve by following simple rules, so any AI failure is unexpected. Indeed, the AI models <strong>performed poorly even on moderate puzzles</strong>: for example, they â€œconsistently failed at Tower of Hanoi,â€ scoring under 80% on a 7-disk puzzle and essentially 0% at 8 disks. Similar breakdowns occurred on the river-crossing and block-stacking puzzles, revealing that these high-profile LLMs could not handle basic logical reasoning.</p><h4>Key Findings</h4><ul><li><strong>Complexity regimes:</strong> The Apple paper identifies three regimes. On very easy tasks, <em>standard</em> LLMs (no extra â€œchain-of-thoughtâ€) actually outperformed the chain-of-thought LRMs; on moderately hard tasks, the LRMs had an edge; on very hard tasks, <em>all</em> models collapsed.</li><li><strong>Complete collapse:</strong> The authors observe a â€œcomplete accuracy collapse beyond certain complexitiesâ€. In other words, once puzzle size passes a threshold, none of the models solved it correctly.</li><li><strong>Specific failures:</strong> In concrete terms, the LLMs <em>â€œconsistently failedâ€</em> the test puzzles. For example, accuracy plunged below 80% on a 7-disk Tower of Hanoi and to ~0% at 8 disks. They also failed the Blocks World and river-crossing puzzles with the same ease as flipping heads on aÂ coin.</li><li><strong>Solution truncation:</strong> The models often stopped mid-solution once answers got long. In practice, a model might begin listing the moves and then insert â€œIâ€™ll stop hereâ€ when the token budget is reached. (Critics note these token limits can mimic a â€œcollapse.â€)</li><li><strong>No explicit algorithms:</strong> Apple found that LRMs â€œfail to use explicit algorithms and reason inconsistently across puzzlesâ€. In short, the modelâ€™s internal â€œthinkingâ€ was erratic rather than systematic.</li></ul><p>These outcomes show that current LLMsâ€™ impressive chain-of-thought outputs can still break down under moderate complexity. Beyond a certain point, giving extra â€œthinking stepsâ€ does <em>not</em> make the models solve problems correctly. In fact, the study concludes that these reasoning models only work up to a limited complexity, after which their performance collapses catastrophically.</p><h4>Implications for AI Understanding</h4><p>Even when LLMs produce elaborate internal â€œreasoning,â€ that process may not be logical. Appleâ€™s co-author Iman Mirzadeh emphasizes that even when the model was <em>given</em> the correct solution algorithm, it â€œstill failedâ€ the puzzleâ€Šâ€”â€Šits process â€œis not logical and intelligentâ€. In other words, the step-by-step chain-of-thought it outputs can be misleading. As MacDailyNews reports, the study suggests these AI models simply â€œgenerate responses that align with pattern-matchingâ€ from their training data, rather than deducing new logic. This echoes Gary Marcusâ€™s point that neural nets can only â€œgeneralize within a distributionâ€ they have seen and â€œtend to break down beyond that distributionâ€. In short, <strong>highly polished AI answers are not proof of genuine understanding</strong>; the models may be drawing on learned patterns instead of applying true reasoning.</p><h4>Critiques and Counterpoints</h4><p>These surprising failures have prompted debate. Open Philanthropy researcher Alex Lawsen argues that the reported â€œcollapseâ€ often reflects evaluation choices, not just model limitations. For example, at the point Apple marked the 8-disk Hanoi as failed, the model was already bumping into its token limit (even printing â€œIâ€™ll stop hereâ€). Lawsen found that if you instead prompt the model to output a concise program (e.g. a recursive algorithm) rather than listing every move, the models easily solve much larger instances (15-disk Hanoi). He also notes Appleâ€™s river-crossing tests included impossible configurations (no solution), so a correct â€œno solutionâ€ answer was scored as a failure. These points suggest the AI was partly failing due to output-format constraints, not only reasoning deficits. Nonetheless, Lawsen agrees that current LLMs still lack robust, generalizable reasoning: truly proving algorithmic intelligence remains an open challenge. In any case, both the original study and its critiques highlight that how we test â€œreasoningâ€ matters. Future benchmarks must separate genuine logical skill from practical outputÂ limits.</p><p>In summary, the Apple puzzle experiments offer a reality check: even cutting-edge LLMs can falter on deceptively simple tasks, underscoring the gap between <em>appearance</em> and <em>understanding</em>. As Futurism puts it, these models â€œare no substitute for good, well-specified algorithms,â€ and their impressive outputs should not be taken as evidence of true intelligence.</p><h3>2. Limits in Logic and Reasoning</h3><p>One key insight from Appleâ€™s research is that current LLMs rely heavily on <strong>pattern recognition</strong>, not true reasoning. They excel when regurgitating information that resembles their training data but falter when faced with novelty. As complexity rises or context shifts, their logical scaffolding crumbles.</p><p>This means we are still far from AIs that can <em>reason</em> through real-world problems in flexible, adaptiveÂ ways.</p><h3>3. Superintelligence Trust Outpaces Capabilities</h3><p>Tech leaders like Sam Altman (OpenAI), Demis Hassabis (Google DeepMind), and Dario Amodei (Anthropic) have publicly speculated about the nearness of AGI. But Appleâ€™s findingsâ€Šâ€”â€Šand corroborating critiquesâ€Šâ€”â€Šsuggest that these forecasts are <strong>leaps ahead of actual performance</strong>.</p><p>While marketing claims lean toward inevitability, the modelsâ€™ analytical capabilities lag significantly behind expectations. Trust in â€œsuperintelligenceâ€ is growing faster than the techâ€™s actual reasoning abilities.</p><h3>4. Emergent Tasks â‰  General Reasoning</h3><p>Some AI advocates point to â€œemergent behaviorâ€ as evidence of intelligenceâ€Šâ€”â€Štasks the model seems to solve despite no direct training. Yet, <strong>emergence doesnâ€™t equal generality</strong>.</p><p>These systems often display narrow brilliance that doesnâ€™t transfer outside very specific formats. Being good at solving math benchmarks or chess problems doesnâ€™t mean the model can reason abstractly or solve unfamiliar, open-ended questions.</p><h3>5. Memory Without Comprehension</h3><p>These models are <strong>masters of mimicry</strong>, not meaning. They recall and remix text without true comprehensionâ€Šâ€”â€Šan issue compared by some researchers to a â€œstochastic parrot.â€ They can ace standardized tests by parroting patterns but lack conceptual frameworks.</p><p>This leads to superficial results: a model that can draft a research summary or pass a quiz may not â€œunderstandâ€ anything itÂ says.</p><h3>6. Pattern vs. Reason: Hybrid Intelligence Is a SmarterÂ Goal</h3><p>Emerging academic consensus suggests that scaling current models may <strong>not</strong> bring us closer to genuine intelligence. Instead, <strong>hybrid intelligence</strong>â€Šâ€”â€Šsystems combining AIâ€™s computational power with human intuition and conceptual reasoningâ€Šâ€”â€Šmay be the more realistic and safe trajectory.</p><p>Rather than replacing humans, future systems could <strong>collaborate</strong> with us, complementing our strengths rather than mimicking our thinking.</p><h3>7. Superintelligence Raises Complex ControlÂ Problems</h3><p>The theoretical risks of superintelligent systemsâ€Šâ€”â€Šlike alignment failure and existential threatsâ€Šâ€”â€Šhave been explored by scholars like Nick Bostrom. But Appleâ€™s study reinforces that weâ€™re <strong>nowhere near that threshold</strong>.</p><p>We may one day face such control dilemmas, but right now, a bigger problem is the <strong>overconfidence in AIâ€™s current capabilities</strong>â€Šâ€”â€Ša misunderstanding that could lead to careless deployment or misplaced trust.</p><h3>8. Policy Implications DemandÂ Realism</h3><p>As governments rush to regulate AI, <strong>misjudging its capabilities could backfire</strong>. Overestimating what these systems can do may lead to inappropriate legal frameworks, poorly allocated funding, or even political fear-mongering.</p><p>Effective policy should be rooted in <strong>what AI actually is</strong>, not in sci-fi predictions. Appleâ€™s research urges a recalibration of the narrative.</p><h3>9. Household Tasks Donâ€™t Equal Awareness</h3><p>Writing a blog post, generating a spreadsheet formula, or replying to emails may seem intelligentâ€Šâ€”â€Šbut none of this requires <strong>awareness or understanding</strong>. Itâ€™s the <em>illusion</em> of thinking, not thinkingÂ itself.</p><p>Todayâ€™s systems appear useful not because they grasp meaning, but because theyâ€™ve digested billions of examples. Thatâ€™s not intelligenceâ€Šâ€”â€Šitâ€™s compression.</p><h3>10. Hype vs. Reality: Why ScrutinyÂ Matters</h3><p>Calling out AIâ€™s limitations isnâ€™t being cynicalâ€Šâ€”â€Š<strong>itâ€™s necessary</strong>. We canâ€™t build responsible systems if weâ€™re blinded by buzzwords. Appleâ€™s work is a reminder: <strong>understanding where we are helps ensure we go further, better, andÂ safer</strong>.</p><h3>Thank you for being a part of the community</h3><p><em>Before youÂ go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writerÂ ï¸ğŸ‘<strong>ï¸ï¸</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> |Â <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a>Â ğŸš€</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a>Â ğŸ§‘ğŸ»â€ğŸ’»</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=69018636646e" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/superintelligence-isnt-imminent-and-the-illusion-of-thinking-69018636646e">ğŸ§  Superintelligence Isnâ€™t Imminentâ€Šâ€”â€ŠAnd The Illusion of Thinking.</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>Vibe Coding: A New Kind of Coding.â€Šâ€”â€ŠPart 1/5</title>
            <link>https://frankcozzolino.github.io/blog.html?article=vibe-coding-a-new-kind-of-coding-part-15</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=vibe-coding-a-new-kind-of-coding-part-15</guid>
            <pubDate>Mon, 16 Jun 2025 10:26:53 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>ai-programming</category>
            <category>vibe-coding</category>
            <category>prompt-engineering</category>
            <category>ai-productivity</category>
            <category>llm-agent</category>
            <description>Vibe Coding: A New Kind of Coding. â€” Part 1/5Vibe coding is a high-level framework that abstracts the complexity of GPU and parallel programming, making CUDA kernels and thread management accessible...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/0*Yv2vRwl-OX80wLDA" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3><strong>Vibe Coding: A New Kind of Coding.â€Šâ€”â€ŠPartÂ 1/5</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Yv2vRwl-OX80wLDA" /></figure><blockquote>Vibe coding is a high-level framework that abstracts the complexity of GPU and parallel programming, making CUDA kernels and thread management accessible even to developers without deep specialized knowledge. By auto-optimizing memory transfers and kernel configurations, it delivers both higher performance and reduced energy consumption. Coupled with extensive UML-based design modeling, covering class, sequence, and deployment diagrams, teams can identify security risks early, enforce robust architectures, and minimize implementation flaws. Together, these approaches democratize high-performance computing, lower operational costs, and drive humanity toward faster, greener, and more secure software innovation.</blockquote><p>Thereâ€™s never much room for feeling in the cold logic of software engineering. Programs either run or they donâ€™t. Brackets match or they donâ€™t. The compiler doesnâ€™t care how inspired you felt at 2 a.m. But in 2025, a different kind of code is emergingâ€Šâ€”â€Šnot just from logic gates, but from architectural vision and natural language. Itâ€™s structured, holistic, and principled. And itâ€™s called vibeÂ coding.</p><p>While originally coined by Andrej Karpathy in a whimsical tweet, the term has since evolved far beyond its tongue-in-cheek roots. No longer shorthand for improvisational tinkering, vibe coding now represents a movement: a way of programming that empowers developers to zoom out and orchestrate systems, rather than wrestle endlessly with syntax andÂ bugs.</p><p>At its core, vibe coding is about clarity. It leverages AI not to bypass engineering, but to elevate it. By translating architectural plans into executable code, developers reclaim time lost in repetitive problem-solving. Instead of fixing broken imports or hunting for elusive null pointer exceptions, theyâ€™re spending that time crafting better user experiences, embedding resilience into their systems, and refining their securityÂ posture.</p><p>This is why UML diagrams and detailed technical design have become essential tools in vibe coding workflows. They serve as the blueprint from which all code generation is orchestrated. In the world of vibe coding, a well-structured technical plan isnâ€™t a nice-to-haveâ€Šâ€”â€Šitâ€™s the beating heart of the process. These visual schematics help align teams, inform prompts, and ensure consistency across modules, APIs, and data flows. The stronger the architecture, the more precise the AIâ€™s contribution.</p><p>It works through prompt-driven development. The developer writes natural language descriptions of the behavior or architecture they want. AI agentsâ€Šâ€”â€Šbe it Cursor, Copilot, Claude, or custom toolingâ€Šâ€”â€Šgenerate code that matches the spec. But the human role doesnâ€™t end there. Itâ€™s about guiding, auditing, and integrating that output with foresight and discipline. In short: vibe coding isnâ€™t hands-offâ€Šâ€”â€Šitâ€™s hands-on, but eyesÂ up.</p><p>Vibe coding redefines who the architect is. No longer does architectural thinking reside solely with senior engineers or system designers. With the right AI interfaces, even junior developers or domain experts can outline workflows, enforce constraints, and maintain system-wide integrity. This democratization of high-level design is changing the shape of teams and the nature of software planning.</p><p>Security is no afterthought in this world. One of Vibe Codingâ€™s greatest promises lies in its ability to embed secure defaults. Instead of retrofitting authorization rules or validating inputs after vulnerabilities are discovered, developers can encode policies into their prompts from the outset. â€œDesign the API to reject malformed JWT tokens,â€ one prompt might say. Or, â€œOnly allow data access if the userâ€™s session is active and within a specified role.â€ The AI generates these flows in real-time, weaving security into the applicationâ€™s fabric.</p><p>Critics still point fingers. They argue that AI-assisted development encourages laziness, or that it leads to black-box software, code nobody understands. But practitioners push back. They argue that the most responsible use of vibe coding is deeply intentional. The point isnâ€™t to abandon engineering; itâ€™s to sharpen it. To use AI to remove friction, notÂ thought.</p><p>â€œI used to spend days debugging cascading failures caused by brittle test setups,â€ says Gabriel Nassar, a software architect at a cybersecurity firm. â€œNow, I describe my resilience strategy up front, and the AI handles scaffolding. I still validate and test, but Iâ€™m not stuck in the weeds. Iâ€™m focused on the mission.â€</p><p>This shift is also freeing smaller teams to do more with less. By offloading time-consuming boilerplate tasks to AI, vibe coding gives developers more time to focus on rigorous testing. Small teams that once struggled to cover edge cases can now afford to build comprehensive test suites and validate their systems with greater confidence.</p><p>More importantly, vibe coding is helping redefine where engineering excellence can come from. In regions like Europe and the United States, where labor costs are higher and outsourcing has long been seen as a cost-saving necessity, vibe coding offers a new path. It allows local engineers to compete at scale, delivering sophisticated, high-integrity software systems without bloated headcounts or offshore labor. By automating the mundane, vibe coding lets domestic teams focus on quality, innovation, and architecture.</p><p>The tools themselves are maturing. AI models are learning to explain their logic, suggest tests, and integrate with CI/CD systems that enforce code health. Version-controlled prompt chainsâ€Šâ€”â€Šcomplete with architectural diagrams and security annotationsâ€Šâ€”â€Šare replacing scattered README files and spaghetti docstrings.</p><p>And perhaps most importantly, vibe coding is cultivating a new programming literacy. One that prizes systems thinking over brute force. One that empowers teams to move faster, not by skipping steps, but by making every step more intelligent.</p><p>The future wonâ€™t be built by AI alone. But it may be shaped by those who know how to harness itâ€”not recklessly, but withÂ intent.</p><p>Thatâ€™s the realÂ vibe.</p><h3>Thank you for being a part of the community</h3><p><em>Before youÂ go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writerÂ ï¸ğŸ‘<strong>ï¸ï¸</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> |Â <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a>Â ğŸš€</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a>Â ğŸ§‘ğŸ»â€ğŸ’»</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=983bc651fbd7" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/vibe-coding-a-new-kind-of-coding-part-1-5-983bc651fbd7">Vibe Coding: A New Kind of Coding.â€Šâ€”â€ŠPart 1/5</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>OpenAIâ€™s o3-Pro Outranks PhDsâ€Šâ€”â€ŠNow the Experts Are Worried</title>
            <link>https://frankcozzolino.github.io/blog.html?article=openais-o3pro-outranks-phds-now-the-experts-are-wo</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=openais-o3pro-outranks-phds-now-the-experts-are-wo</guid>
            <pubDate>Sat, 14 Jun 2025 13:39:08 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>data-science</category>
            <category>artificial-intelligence</category>
            <category>technology</category>
            <category>science</category>
            <category>education</category>
            <description>OpenAIâ€™s o3-Pro Outranks PhDs â€” Now the Experts Are WorriedPeople across industry, academia, and social media have lauded OpenAIâ€™s o3-pro for its exceptional performance on graduate-level science...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*PTk3xTRiMztBXmkwMylknQ.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h2><strong>OpenAIâ€™s o3-Pro Outranks PhDsâ€Šâ€”â€ŠNow the Experts AreÂ Worried</strong></h2><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PTk3xTRiMztBXmkwMylknQ.png" /></figure><p>People across industry, academia, and social media have lauded OpenAIâ€™s o3-pro for its exceptional performance on graduate-level science benchmarksâ€Šâ€”â€Šmost notably the GPQA Diamond testâ€Šâ€”â€Šwhere it not only surpasses human expert baselines but also outperforms rival models like Googleâ€™s Gemini 2.5 Pro and Anthropicâ€™s Claude 4 Opus, all while demonstrating deep domain reasoning that reflects PhD-level scientific acumen.Â . At the same time, academic researchers caution that these benchmark victories, achieved through massive trialling of predefined operations, may not fully capture generalizable understanding, underscoring the need for broader evaluation frameworks and vigilant oversight as o3-pro begins to power real-world research and decision-making workflows.Â .</p><h3>Benchmark Breakthroughs in PhD-Level Science</h3><p>â€œOn PhD-level science questions on the GPQA Diamond benchmark, [o3-pro] scored 84%, again surpassing its predecessors,â€ reports Cogni Down Under, reflecting a leap above the 69.7% average achieved by human experts with PhDs on the same dataset.Â . TechCrunch similarly notes that o3-pro â€œbeats Anthropicâ€™s recently released Claude 4 Opus on GPQA Diamond, a test of PhD-level science knowledgeâ€ while also outperforming Googleâ€™s Gemini 2.5 Pro on AIME 2024, a rigorous mathematics exam.Â .</p><h3>Academic Perspectives on Expert-Level Reasoning</h3><p>The GPQA Diamond subset contains 198 multiple-choice questions crafted by domain experts pursuing or holding PhDs in biology, physics, and chemistry, with a random-guess baseline of 25% and a human expert baseline of 69.7%Â . Yet, Rolf Pfister and Hansueli Jud warn that o3â€™s record performance â€œraises the question whether systems based on LLMs demonstrate genuine intelligence,â€ since the model achieves high scores via extensive brute-force trialling rather than true conceptual understanding.Â . Similarly, reflective benchmarks have shown that while o3-proâ€™s â€œprivate chain of thoughtâ€ yields impressive accuracy, longer response times and reliance on predefined operations may limit its adaptability to novel, unstructured scientific challenges.Â .</p><h3>Expert Reactions and Practical UseÂ Cases</h3><p>Ethan Mollick, a leading AI researcher, shared on LinkedIn:</p><p>â€œBeen playing with o3-pro for a bit. It is quite smart. One problem it solved where every other model has failed is making a word ladder from SPACE toÂ EARTH.â€</p><p>David Borish, AI strategist at Trace3, emphasizes that o3-proâ€™s step-by-step reasoning makes it â€œparticularly effective for complex tasks in mathematics, science, and engineering contextsâ€Â . Early adopters in research labs report using o3-pro to draft detailed grant proposals, analyze experimental datasets, and generate hypothesis-driven literature reviewsâ€Šâ€”â€Šworkflows traditionally reserved for senior PhD candidates.Â .</p><h3>Case Study: University Exam Performance</h3><p>In a striking demonstration of its foundational capabilities, a recent study found that the predecessor model o3 aced a zero-shot university thermodynamics examâ€Šâ€”â€Šscoring perfectly and outperforming the top studentsâ€Šâ€”â€Šhighlighting the genuine academic rigor that o3-pro inherits and extends for advanced scientific problem solving.Â .</p><h3>Balancing Breakthroughs with Oversight</h3><p>Despite widespread praise, critics warn of erratic behavior and â€œjagged frontierâ€ unpredictability in advanced AI models, noting occasional math errors and overconfidence in unfamiliar domains.Â . As o3-pro takes center stage in PhD-level workflows, experts agree that robust validation, diversified benchmarks, and human-in-the-loop oversight will be essential to harness its full potential responsibly.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f8ad4e5483e" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
    </channel>
</rss>