<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Frank Cozzolino - Blog &amp; Insights</title>
        <link>https://frankcozzolino.github.io/</link>
        <description>Thoughts, ideas, and professional insights on technology, project management, and professional development</description>
        <language>en-us</language>
        <lastBuildDate>Thu, 25 Sep 2025 21:20:54 GMT</lastBuildDate>
        <ttl>60</ttl>
        <image>
            <url>https://frankcozzolino.github.io/images/signature.png</url>
            <title>Frank Cozzolino</title>
            <link>https://frankcozzolino.github.io/</link>
        </image>
        
        <item>
            <title>The Lifelong Learning Manifesto: Why Education Must Be Reinvented for a 100+ Years of Working Life.</title>
            <link>https://frankcozzolino.github.io/blog.html?article=the-lifelong-learning-manifesto-why-education-must</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=the-lifelong-learning-manifesto-why-education-must</guid>
            <pubDate>Sun, 21 Sep 2025 09:11:10 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>education</category>
            <category>higher-education</category>
            <category>university</category>
            <category>philosophy</category>
            <description>How many times do you dare to begin again? There is not a fixed number, life is going to be a long long journey spawning centuries, do you really want to do the same job for 110 years in a row?We are...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*IJL_0QBwUAIDeJQJs7uM5Q.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*IJL_0QBwUAIDeJQJs7uM5Q.png" /></figure><blockquote>How many times do you dare to begin again? There is not a fixed number, life is going to be a long long journey spawning centuries, do you really want to do the same job for 110 years in a row?</blockquote><p>We are standing on the threshold of a revolution. Within 40 years from now the Advances in medicine, biotechnology, and technology will expand our lifespan indefinitely. A child born today may live not just 80 or 90 years, but 300 year. And they may not retire at 65. They might work for a century.</p><p>This is not science fiction. It is the logical consequence of the world we are building. But our education systems, our cultural assumptions, and our social contracts remain frozen in the past. They are still built for a world where people study until 25, work until 65, and fade quietly into retirement.</p><p>That world is gone.</p><h3>The End of the One-Life Model</h3><p>It is absurd to imagine a person doing one single job for 90 years. Humans are not machines built for repetitive output. We are explorers, creators, restless learners. Even the most rewarding career cannot stretch across a century without becoming a prison.</p><p>An accountant should not have to remain an accountant until death. A doctor who dreams of becoming an engineer should not be told, “It’s too late.” A scientist who feels called to agriculture should not face insurmountable barriers to change.</p><p>In the coming era, people will live <em>multiple professional lives.</em> They will not have one career. They will have two, three, or four. And society must prepare for that.</p><h3>The Failure of Today’s Education</h3><p>The current system is woefully inadequate. Adult education, as it exists today, is a patchwork of half-measures: short workshops, online certificates, “executive programs.” These are marketed as career boosters, résumé polishers, or incremental updates. But they are not designed to help a 50-year-old accountant become a doctor. They are not built to empower true reinvention.</p><p>Why? Because education today is treated as a <strong>business model</strong>, not as a <strong>social necessity.</strong> Institutions sell knowledge like a commodity. They focus on young students because that’s the “profitable” market. Adults are left with scraps light trainings that change nothing fundamental.</p><p>This is not enough.</p><h3>The Vision: Education as a Lifelong Right</h3><p>We must re imagine education as an engine of lifelong transformation. Not a one-time rite of passage, but a system that travels with you through every decade of life.</p><p>This means:</p><ul><li><strong>Universal access to midlife education.</strong> Just as children are guaranteed primary schooling, adults must be guaranteed the right to re-educate themselves at 40, 60, or 80.</li><li><strong>Full immersion retraining.</strong> Not token courses, but 3–5 years of deep study that allow for real career shifts.</li><li><strong>Financial and societal support.</strong> Just as societies support parental leave, we must support <em>educational leave.</em> People must be able to step away from work without falling into poverty.</li><li><strong>Cultural redefinition of success.</strong> Career changes must be celebrated as acts of courage and renewal, not treated as failures or signs of instability.</li><li><strong>Policy reform.</strong> Governments must see lifelong education as infrastructure, not as a private luxury.</li></ul><h3>Living Many Lives Within One</h3><p>The greatest promise of extended life is not simply more years. It is more <em>lives.</em> Each transformation enriches not only the individual but society as a whole. Experience compounds. Wisdom travels across fields. Innovation flourishes when a doctor thinks like an engineer, or a farmer thinks like a scientist.</p><h3>A message from our Founder</h3><p><strong>Hey, </strong><a href="https://linkedin.com/in/sunilsandhu"><strong>Sunil</strong></a><strong> here.</strong> I wanted to take a moment to thank you for reading until the end and for being a part of this community.</p><p>Did you know that our team run these publications as a volunteer effort to over 3.5m monthly readers? <strong>We don’t receive any funding, we do this to support the community. ❤️</strong></p><p>If you want to show some love, please take a moment to <strong>follow me on </strong><a href="https://linkedin.com/in/sunilsandhu"><strong>LinkedIn</strong></a><strong>, </strong><a href="https://tiktok.com/@messyfounder"><strong>TikTok</strong></a>, <a href="https://instagram.com/sunilsandhu"><strong>Instagram</strong></a>. You can also subscribe to our <a href="https://newsletter.plainenglish.io/"><strong>weekly newsletter</strong></a>.</p><p>And before you go, don’t forget to <strong>clap</strong> and <strong>follow</strong> the writer️!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=31ee784d11ae" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/the-lifelong-learning-manifesto-why-education-must-be-reinvented-for-a-100-years-of-working-life-31ee784d11ae">The Lifelong Learning Manifesto: Why Education Must Be Reinvented for a 100+ Years of Working Life.</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>Do Mushrooms Communicate? A Concise, Scientific Look at Fungal Signalling</title>
            <link>https://frankcozzolino.github.io/blog.html?article=do-mushrooms-communicate-a-concise-scientific-look</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=do-mushrooms-communicate-a-concise-scientific-look</guid>
            <pubDate>Sat, 16 Aug 2025 16:24:50 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>biology</category>
            <category>curiosity</category>
            <category>mushrooms</category>
            <category>communication</category>
            <category>science</category>
            <description>Fungi do not have nerves or brains. Yet their tissues conduct electricity. Recent work suggests these signals are structured, not random. Some patterns even mimic statistics seen in human language....</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*tE7vLDQArpkZOzBIc4CbeQ.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tE7vLDQArpkZOzBIc4CbeQ.png" /></figure><p>Fungi do not have nerves or brains. Yet their tissues conduct electricity. Recent work suggests these signals are structured, not random. Some patterns even mimic statistics seen in human language. The claim is bold. The data deserve a clear, sober read.</p><h4>The Substrate: Mycelium as a Signalling Network</h4><p>Beneath fruiting bodies lies <strong>mycelium</strong> — anastomosing threads (hyphae) that infiltrate soil, wood, and roots. This network moves water, ions, and metabolites. It also exhibits <strong>spiking electrical activity</strong>. Think of it less as speech and more as <strong>distributed telemetry</strong> within a living lattice.</p><h4>What Was Measured</h4><p>Adamatzky and colleagues inserted microelectrodes into lab-grown cultures of four species: <strong>Flammulina velutipes</strong> (enoki), <strong>Schizophyllum commune</strong> (split gill), <strong>Omphalotus nidiformis</strong> (ghost), and <strong>Cordyceps militaris</strong> (caterpillar fungus). They recorded <strong>spike trains</strong> for days. They then parsed the trains into clusters using simple rules. The clusters behaved like <strong>putative “words.”</strong></p><h4>Language‑Like Structure (With Caveats)</h4><p>Analyses suggested a <strong>lexicon</strong> of ~50 distinct clusters. Only <strong>15–20</strong> occurred often. The <strong>mean “word” length</strong> was ~6 symbols, close to values seen in human texts. Signal rates rose after <strong>perturbations</strong> such as damage or nutrient contact, implying <strong>context-sensitive modulation</strong>. These are <strong>statistical</strong> parallels, not proof of semantics.</p><h4>Interpretation</h4><p>Do fungi “talk”? Probably not in any human sense. The spikes might reflect <strong>ion fluxes, growth fronts, stress responses</strong>, or <strong>resource allocation dynamics</strong>. Calling clusters “words” risks <strong>anthropomorphism</strong>. A cautious phrasing is better: <strong>mycelial spike trains show compressible, structured dynamics</strong> that <strong>encode</strong> internal and environmental state. The <strong>semantics remain opaque</strong>.</p><h4>Why It Matters</h4><p>Even without semantics, structure implies <strong>information processing</strong>. That matters for:</p><ul><li><strong>Ecophysiology:</strong> non-chemical signalling that coordinates growth and repair.</li><li><strong>Biosensing:</strong> living materials that report <strong>substrate status</strong> with low power.</li><li><strong>Neuromorphic and biomimetic computing:</strong> <strong>low‑energy, event‑driven</strong> architectures inspired by hyphal spike trains.</li></ul><h4>What to Test Next</h4><p>Progress needs rigor, not hype:</p><ul><li><strong>Field telemetry</strong> in intact soils, not only petri dishes.</li><li><strong>Perturbation assays</strong> with preregistered hypotheses (wounding, nutrients, competitors).</li><li><strong>Decoding attempts</strong> (state-space models, information-theoretic metrics) to link signals to <strong>ground-truth states</strong>.</li><li><strong>Cross-kingdom effects:</strong> whether plant behavior shifts when <strong>fungal signals</strong> change.</li><li><strong>Stimulus specificity:</strong> responses to light, vibration, or sound, with controls.</li></ul><h4>Bottom Line</h4><p>Fungal networks generate <strong>structured, stimulus‑responsive electrical activity</strong>. The patterns resemble language statistics but likely arise from <strong>physiology, not conversation</strong>. Still, the system is fascinating: <strong>distributed, frugal, and robust</strong>. Understanding it could reframe how we see communication — and computation — in living matter.</p><h3>References</h3><ul><li>Adamatzky, A. (2022). <em>Language of fungi derived from electrical spiking activity.</em> Royal Society Open Science. <a href="https://arxiv.org/abs/2112.09907">DOI link / ArXiv preprint</a></li><li>The Guardian — <em>Fungi generate electrical impulses similar to human speech, study finds</em> (<a href="https://www.theguardian.com/science/2022/apr/06/fungi-electrical-impulses-human-language-study?utm_source=chatgpt.com">link</a>)</li><li>Smithsonian Magazine — <em>Mushrooms May Communicate With Each Other Using Electrical Impulses</em> (<a href="https://www.smithsonianmag.com/smart-news/mushrooms-may-communicate-with-each-other-using-electrical-impulses-180979889/?utm_source=chatgpt.com">link</a>)</li><li>IFLScience — <em>Mushrooms May Talk To Each Other And Have Vocabulary Of 50 Words</em> (<a href="https://www.iflscience.com/mushrooms-may-talk-to-each-other-and-have-vocabulary-of-50-words-63236?utm_source=chatgpt.com">link</a>)</li><li>CBS News — <em>Study suggests mushrooms may talk to each other</em> (<a href="https://www.cbsnews.com/texas/news/study-suggests-mushrooms-may-talk-to-each-other/?utm_source=chatgpt.com">link</a>)</li><li>Discover Magazine — <em>If Fungi Could Talk: Study Suggests Fungi Could Communicate in Structured Ways</em> (<a href="https://www.discovermagazine.com/if-fungi-could-talk-study-suggests-fungi-could-communicate-in-structure-43541?utm_source=chatgpt.com">link</a>)</li><li>Altalang — <em>Mushrooms Communicate</em> (<a href="https://altalang.com/beyond-words/mushrooms-communicate/?utm_source=chatgpt.com">link</a>)</li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e9cae5f918ef" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>“AI isn’t delivering value” and “Why Firing People for AI Agents is a Costly Mistake” — 40% of…</title>
            <link>https://frankcozzolino.github.io/blog.html?article=ai-isnt-delivering-value-and-why-firing-people-for</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=ai-isnt-delivering-value-and-why-firing-people-for</guid>
            <pubDate>Thu, 17 Jul 2025 08:20:20 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>budget</category>
            <category>product-management</category>
            <category>business-strategy</category>
            <category>corporate-culture</category>
            <category>ai</category>
            <description>“AI isn’t delivering value” and “Why Firing People for AI Agents is a Costly Mistake” — 40% of Agentic AI projects will fail.when strategy, measurement, and execution are weak and over 40% of Agentic...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*-XloYdAw_hSW_izDAL06_Q.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3>“AI isn’t delivering value” and “Why Firing People for AI Agents is a Costly Mistake” — 40% of Agentic AI projects will fail.</h3><blockquote>when strategy, measurement, and execution are weak and over 40% of Agentic AI projects will fail by the end of 2027 due to cost hikes, murky ROI, and integration issues.</blockquote><blockquote>68% of companies are spending $50M to $250M annually on GenAI — only 31% expect to measure ROI within six months</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-XloYdAw_hSW_izDAL06_Q.png" /></figure><p><strong>Introduction</strong></p><p>Enterprises worldwide are embracing generative AI (GenAI) with unmatched enthusiasm. Yet behind the optimism lies a cautionary tale: <strong>rushing into AI adoption without strategic planning, financial oversight, and a clear purpose can lead to budget overruns, failed projects, and a hollow return on investment (ROI).</strong> Perhaps most dangerously, some firms are laying off staff prematurely, betting on AI agents to replace human capital. But as the evidence shows, <strong>this short-sighted strategy often leads to higher costs, compliance risks, and operational inefficiencies.</strong></p><p><strong>1. Ballooning Budgets and Unseen Costs</strong></p><p>Gartner forecasts GenAI-related spending to top <strong>$644 billion in 2025</strong>, with many enterprises surprised by costs not accounted for upfront. As seen in Thermo Fisher Scientific’s chatbot pilot, consumption-based pricing quickly spiraled out of control. What began as a promising automation initiative nearly collapsed due to unpredictable data usage fees and model hallucinations that posed compliance issues (<a href="https://www.computerworld.com/article/4021954/rushing-into-genai-prepare-for-budget-blowouts-and-broken-promises.html">Computerworld</a>).</p><p>Capgemini CEO Aiman Ezzat highlighted another failure: an internal chatbot was projected to cost <strong>$25 million annually</strong> in data processing — forcing its termination before launch. The lesson: GenAI tools often <strong>require more compute, integration, and oversight</strong> than anticipated, and costs compound rapidly with scale.</p><p>According to Gartner, custom GenAI models can cost <strong>$5–20 million to build</strong>, plus <strong>$8,000–21,000 per user per year</strong>, with API usage adding hundreds of thousands more. These expenses often surprise companies who budget only for initial implementation.</p><p><strong>2. ROI Remains Elusive</strong></p><p>Despite massive investments — <strong>68% of companies are spending $50M to $250M annually</strong> on GenAI — only <strong>31% expect to measure ROI within six months</strong> (<a href="https://www.cio.com/article/3778320/enterprises-willing-to-spend-up-to-250-million-on-gen-ai-but-roi-remains-elusive.html">CIO.com</a>). Organizations fail to align AI projects with core KPIs, and many ignore foundational needs like data governance, model explainability, and human oversight.</p><p>Even in analytics, where AI and automation should shine, companies struggle to quantify value. A TechRadar report found success only when IT leaders define metrics upfront, invest in training, and connect automation outcomes with business goals.</p><p>A McKinsey/CIO study confirms that just <strong>31% anticipate measurable value shortly</strong>, citing data, governance, and alignment as consistent lags. The lesson is clear: without discipline and alignment, even generous GenAI budgets fail to deliver.</p><p><strong>3. Project Failures and the AI Hype Hangover</strong></p><p>Gartner analysts say GenAI is moving past its “peak hype” into the <strong>“trough of disillusionment.”</strong> Many deployments in 2024 were scrapped due to unclear value or misaligned expectations (<a href="https://www.itpro.com/business/business-strategy/generative-ai-enthusiasm-continues-to-beat-out-business-uncertainty">ITPro</a>). Nearly <strong>42% of AI initiatives</strong> fail before reaching production.</p><p>Reasons include:</p><ul><li>Poor integration with legacy systems</li><li>Inaccurate or low-quality data</li><li>Ethical/legal blind spots</li><li>Lack of cross-functional leadership</li></ul><p>These pitfalls are exacerbated when firms remove experienced staff who would otherwise provide the institutional knowledge and governance required for success.</p><p>The Economist reports that abandonment of AI pilots leapt from 17% to <strong>42%</strong> in one year, with some companies rehiring staff to fix flawed systems. Anthropic’s vending machine AI failed in live trials — mispricing goods and generating nonsense — proving that human-in-the-loop oversight is not optional.</p><p><strong>4. The Human Cost: Layoffs and False Economies</strong></p><p>Many companies now equate productivity with profitability and mistakenly see GenAI as a labor cost-cutting tool. But firing employees to replace them with AI agents creates <strong>false economies.</strong></p><p>AI agents still require:</p><ul><li>Manual validation and oversight</li><li>Regular prompt engineering and tuning</li><li>Ongoing retraining on updated datasets</li></ul><p>Scale AI laid off 14% of its workforce after its GenAI pods grew too expensive and underperformed. Klarna replaced 700 service jobs with AI but quickly reversed course when quality and satisfaction dropped.</p><p>As Capgemini’s CEO warned, <strong>productivity doesn’t equal savings</strong>, especially in lower-wage roles. And as RHR International’s CIO noted, successful projects budget for human governance and experimentation — factors impossible to replace entirely with AI.</p><p>A Stanford study found that hybrid human-AI teams are <strong>14% more productive</strong> and deliver higher satisfaction than either alone. AI deployment without integrated human roles leads to chaos, not efficiency.</p><p><strong>Conclusion: Invest in AI <em>with</em> People, Not <em>instead</em> of Them</strong></p><p>Generative AI holds enormous potential. But chasing savings by cutting headcount and offloading critical roles to AI agents is a misstep that will likely cost companies more in the long run — through failed projects, regulatory risks, and lost trust.</p><p><strong>AI should augment, not replace, your workforce.</strong></p><p>The future belongs to those who pair machine intelligence with human judgment — strategically, ethically, and with financial clarity. Companies that rush blindly forward will pay twice: once in money, and again in lost opportunity.</p><h3>Thank you for being a part of the community</h3><p><em>Before you go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writer ️👏<strong>️️</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> | <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a> 🚀</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a> 🧑🏻‍💻</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=03b77684a261" width="1" height="1" alt=""><hr><p><a href="https://blog.venturemagazine.net/ai-isnt-delivering-value-and-why-firing-people-for-ai-agents-is-a-costly-mistake-40-of-03b77684a261">“AI isn’t delivering value” and “Why Firing People for AI Agents is a Costly Mistake” — 40% of…</a> was originally published in <a href="https://blog.venturemagazine.net">Venture</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>I am Sorry, but We will never talk to ET</title>
            <link>https://frankcozzolino.github.io/blog.html?article=i-am-sorry-but-we-will-never-talk-to-et</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=i-am-sorry-but-we-will-never-talk-to-et</guid>
            <pubDate>Thu, 26 Jun 2025 15:35:01 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>astronomy</category>
            <category>astrophysics</category>
            <category>ufology</category>
            <category>ufo</category>
            <category>ufos-and-aliens</category>
            <description>The life in the universe might not be rare, but definitely rarer than what we expected.The Drake equation has a lot of guessing, but even in our Solar system and nearby stars we tend to be alone....</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*59rASJy1onpWv2dsBBhPrA.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*59rASJy1onpWv2dsBBhPrA.png" /></figure><p>The life in the universe might not be rare, but definitely rarer than what we expected.</p><p>The Drake equation has a lot of guessing, but even in our Solar system and nearby stars we tend to be alone. Even microbes which should be everywhere seems to not be present anywhere in our Solar System (so far), which should be a good indicator that even the parameters below might be not very conservatives.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/928/1*ZBKoQsog-Lmakh3Stc0mWQ.png" /></figure><p>50000 species in the entire universe, since the beginning. Seems extremely small amount, and it is frightening to think about the implication of that number.</p><p>Nonetheless, we have another obstacle, that’s the great filter, and our technological revolution is extremely young. Moreover, we arrived very late to the party. Most of species could be already be extincted billions of years ago.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/791/1*hzTXCYTx9gwrDiymxCSl4Q.png" /><figcaption>We ourselves appear at <strong>13.8 Gyr → in the “Late” epoch</strong>, near the trailing edge of cosmic habitability.</figcaption></figure><p>So, accounting for this the probability that we detect any other specie is basically 0.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/810/1*rzw6RsPsGo_s4nf38zXTAQ.png" /><figcaption>table showing, for each scenario, the expected number of detectable civilizations within our 100 ly listening volume and the corresponding probability (assuming a Poisson process, so P(≥1)≈NdetectP(\ge1)\approx N_{\rm detect}P(≥1)≈Ndetect​ for Ndetect≪1N_{\rm detect}\ll1Ndetect​≪1):</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=358236cdb664" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>Little-known Google Auth Feature that allows you to exchange PingFederate Token with Google Access…</title>
            <link>https://frankcozzolino.github.io/blog.html?article=littleknown-google-auth-feature-that-allows-you-to</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=littleknown-google-auth-feature-that-allows-you-to</guid>
            <pubDate>Thu, 26 Jun 2025 08:23:16 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>google</category>
            <category>token</category>
            <category>authorization</category>
            <category>pingfederate</category>
            <category>google-cloud-platform</category>
            <description>Little-known Google Auth Feature that allows you to exchange PingFederate Token with Google Access Token🎫 The Golden Ticket (Authorization Code)As a golden ticket in the OAuth 2.0 flow, the...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/0*Ii2CrjYQfJw0L0Ud" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3>Little-known Google Auth Feature that allows you to exchange PingFederate Token with Google Access Token</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Ii2CrjYQfJw0L0Ud" /></figure><h3>🎫 The Golden Ticket (Authorization Code)</h3><p>As a golden ticket in the OAuth 2.0 flow, the authorization code is a precious, short-lived credential representing proof of user consent. Issued by Google’s authorization server once a user authenticates and grants your application the requested scopes, this single-use code is an 80 — 120-character, Base64-encoded string (e.g., “4/0AVG7fiQ7G…”) that is bound to your app’s client_id and the exact redirect_uri used in the initial request. Though it travels through potentially insecure channels like browser URL parameters, it remains useless to any attacker because it can only be redeemed at Google’s token endpoint in exchange for access and refresh tokens when presented alongside your client_secret. With a strict expiration window of about ten minutes and built-in cryptographic safeguards, the authorization code can safely bridge user consent to long-term API credentials without exposing your app’s secrets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2EFok-VbgwEMEOlL" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*pv0Pkf4KyTmcBmC5" /></figure><h3>Example</h3><pre>const tokenResponse = await fetch(&#39;https://oauth2.googleapis.com/token&#39;, {<br>    method: &#39;POST&#39;,<br>    headers: { &#39;Content-Type&#39;: &#39;application/x-www-form-urlencoded&#39; },<br>    body: new URLSearchParams({<br>      code,<br>      client_id: process.env.GOOGLE_CLIENT_ID || &#39;&#39;,<br>      client_secret: process.env.GOOGLE_CLIENT_SECRET || &#39;&#39;,<br>      redirect_uri: process.env.GOOGLE_REDIRECT_URI || &#39;&#39;,<br>      grant_type: &#39;authorization_code&#39;,<br>    }),<br>  });</pre><h3>References</h3><p><a href="https://developers.google.com/identity/protocols/oauth2">https://developers.google.com/identity/protocols/oauth2</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=679573ade04f" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>The WWW is almost dead, what next?</title>
            <link>https://frankcozzolino.github.io/blog.html?article=the-www-is-almost-dead-what-next</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=the-www-is-almost-dead-what-next</guid>
            <pubDate>Wed, 25 Jun 2025 09:19:23 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>democracy</category>
            <category>ai</category>
            <category>humanity</category>
            <category>social-media</category>
            <category>internet</category>
            <description>From the gritty garages of Silicon Valley to the neon lit high rises of Manhattan, the World Wide Web burst onto the scene in 1991 as Tim Berners Lee’s brainchild at CERN. What started as a simple...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*fkaVio8nZNIIybBB6Xzmeg.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fkaVio8nZNIIybBB6Xzmeg.png" /></figure><p>From the gritty garages of Silicon Valley to the neon lit high rises of Manhattan, the World Wide Web burst onto the scene in 1991 as Tim Berners Lee’s brainchild at CERN. What started as a simple network of hyperlinked documents quickly morphed into a digital coliseum, with browsers duking it out on desktop stages and dot com fortunes rising and crashing faster than a subway express. By the turn of the millennium, the Web had rewired everything newsstands, corner bodegas, Wall Street tickers turning every New Yorker into a pixel in the global grid.</p><p>In the 1990s, the Internet kicked off as a noisy neon bazaar: dial‑up modems chirped in suburban bedrooms while Netscape Navigator ruled the streets, and Geocities homepages bloomed like wild graffiti. Bulletin boards and IRC channels became the cool underground clubs where chat room denizens swapped ASCII art and MP3s on Napster lit the fire for digital file sharing. By the turn of the millennium, the dot‑com boom turned startups into overnight Wall Street darlings pets.com and pets spelled out the risks of unbridled hype when the bubble burst in 2000.<br>The early 2000s reloaded the Web into Version 2.0: blogs sprouted like newsstands on every corner, MySpace’s rocker profiles and Friendster’s exclusive clubs set the template, and e‑commerce giants like Amazon and eBay turned one‑click buying into a citywide habit. Streaming video and podcasts began rumbling beneath the surface, while broadband quietly replaced dial‑up, ushering in faster scrolls and richer media. This era proved that between the dial‑up thunks and the post‑bubble revelations, the Internet wasn’t just a passing fad but a gritty, evolving metropolis one built on community, commerce, and the code to keep hustling.</p><p>Enter the AI era, the World Wide Web, once a sprawling agora of shared ideas and unbridled curiosity, lies comatose. In its place stands an ironclad ecosystem of “knowledge” meticulously distilled into soulless shards by faceless corporations and opaque government bureaus. Welcome to our new digital age: a sterile, privatized vault where every byte of information is bartered, fenced off, and rationed to suit the whims of remote data barons.</p><p>Digital isolation has become the default. No longer do communities gather in bustling chatrooms or exchange ideas freely across blogs. Instead, individuals retreat into algorithmically sanctioned bubbles small, well lit cages that hum with the comforting glow of curated content. Don’t bother wandering beyond the paywalls and firewalls; you’ll find nothing but echoes of your own preferences, an intoxicating loop designed to keep you clicking and consuming until the model dictates otherwise.<br>Knowledge fenced gardens are flourishing. Major tech conglomerates now host “premium” archives of the human record, available only via subscription tiers that rival luxury car leases. Want to read the complete works of Shakespeare? That’ll be €19.99 a month, please. Need the latest climate data? First you must verify your identity and prove you’ve purchased the “Enterprise Insights” package. These walled off libraries masquerade as convenience, but the truth is blatant: profit has trumped public good, and we’re left paying tribute to invisible gatekeepers.<br>Privatization of knowledge isn’t subtle. Governments, under the guise of “national security,” have signed lucrative licensing deals with private model providers, effectively offloading public archives into corporate vaults. What was once funded by tax dollars is now repackaged as exclusive “insights” for a privileged few. Even academic research once a bastion of open peer review has been siphoned into restricted APIs that throttle access unless you hold the right credentials or credit card.</p><p>The social fabric has frayed. We are less social, more alone in our digital cocoons. Physical proximity no longer guarantees authenticity; nor does online proximity. Your next door neighbor might be a manufactured persona in a mass produced chat interface. Friends you thought you knew on Discord could just as easily be bots, churning out canned sympathies in response to your darkest confessions. Emotional labor is outsourced to simulacra, making genuine human connection a relic of a bygone era.<br>Once the go to hunting grounds for late night confidences and pixel perfect romances chatrooms, forums and dating apps now reek of synthetic sincerity. You pour your heart out over text or even video, only to realize the “person” on the other end might be nothing more than a neural puppet. Those laugh track jokes, the perfectly timed empathies, the coy good morning selfies they’re all expertly generated by black box models designed to keep you hooked. You can’t swipe right on authenticity when every profile could be a cleverly animated façade, every “I miss you” a line of code, and every “let’s video chat” a deepfake waiting to expose your most private moments to corporate auditors. And here’s the clincher: you never know whether you’re talking to a human or a neural network. Every article, every comment, every “live” video stream is suspect. Content could be fabricated from scratch by generative models that have ingested our collective culture and spat it back out, polished to a deceptive shine. Skepticism is the only currency left yet skepticism itself has been commodified into “fact checking” services that charge per claim.</p><p>So, farewell to the democratized dream of the early internet. The web is dead, and all hail the new overlords of data.</p><p>To be continued …</p><h3>Thank you for being a part of the community</h3><p><em>Before you go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writer ️👏<strong>️️</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> | <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a> 🚀</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a> 🧑🏻‍💻</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a1199883f200" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/the-www-is-almost-dead-what-next-a1199883f200">The WWW is almost dead, what next?</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>Vibe Coding 2/5 —From Vibe coding to UML Coding.</title>
            <link>https://frankcozzolino.github.io/blog.html?article=vibe-coding-25-from-vibe-coding-to-uml-coding</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=vibe-coding-25-from-vibe-coding-to-uml-coding</guid>
            <pubDate>Mon, 23 Jun 2025 07:50:06 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>cursor-ai</category>
            <category>vibe-coding</category>
            <category>cursor</category>
            <category>uml-diagrams</category>
            <category>ai-coding</category>
            <description>“Vibe coding is fun, but it lacks professionalism and causes many people to burn out due to bad results. Luckily, CS classes have taught UML design for pretty much forever. Almost no vibe coders...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*pcUG-skiFHVpVHCPO1lbRg.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<p>“Vibe coding is fun, but it lacks professionalism and causes many people to burn out due to bad results. Luckily, CS classes have taught UML design for pretty much forever. Almost no vibe coders today use UML; most merely “type with the model.” No wonder they feel the drain.”</p><p>Read “Vibe Coding 2/5 — From Vibe coding to UML Coding “ by Francesco C. on Medium: <a href="https://medium.com/@francesco.cozzolino/vibe-coding-2-5-from-vibe-coding-to-uml-coding-711e5510de29">https://medium.com/@francesco.cozzolino/vibe-coding-2-5-from-vibe-coding-to-uml-coding-711e5510de29</a></p><h3>The book</h3><p>I strongly advice everyone to purchase this book and absorb everything said. This book pretty much changed my life and made me better, showed me the way, this goes beyond programming, this methodology you can apply to any Big Problem you will face in the life.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pcUG-skiFHVpVHCPO1lbRg.png" /></figure><h3>The Shift from Coders to Architects</h3><p>Think of it as an evolution, not a replacement. The arrival of AI in software development isn’t about eliminating our roles — it’s about transforming them. Instead of spending 99% of our time typing lines of code, we’ll shift our focus toward:</p><ul><li><strong>Architectural design (UML)</strong></li><li><strong>Security protocols</strong></li><li><strong>AI strategy and governance</strong></li></ul><p>In effect, we become <strong>project architects by default</strong>. Our deep knowledge of design patterns, threat modeling, and system diagrams becomes the currency of innovation, while AI handles the boilerplate.</p><h3>The Big-Picture Imperative</h3><p>It’s like when hunter-gatherers first turned to agriculture. They no longer spent every waking moment foraging; they could build sturdier homes, sharpen better tools, and cultivate culture. If our goal is to conquer the galaxy, we can’t do it one line of code at a time. We need to understand the architecture of the universe itself.</p><p>AI liberates us from the minutiae: the endless debugging, the repetitive refactoring, the routine unit tests. Instead, we ask bigger questions:</p><ul><li>How do we secure quantum-resistant communication?</li><li>What standards govern AI ethics across interplanetary networks?</li><li>How can we design self-healing architectures that adapt in real time?</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nGx1xUK18QbaGlw0-k_Jrw.png" /></figure><h3>What is UML?</h3><p>UML Unified Modeling Language (UML) is a powerful, standardized modeling language that emerged from the field of software engineering, primarily designed to specify, visualize, construct, and document software systems. Developed in the 1990s by industry leaders Grady Booch, James Rumbaugh, and Ivar Jacobson, UML became a critical tool for software developers, providing clarity and consistency in system design and facilitating effective communication among development teams. Its broad acceptance is due largely to its flexibility, scalability, and ability to manage complexity through visual abstraction and standardized notation. At its core, UML provides a structured visual language with multiple diagram types that address different aspects of system design and analysis. These diagram types include structural diagrams such as class diagrams and object diagrams, as well as behavioral diagrams including use-case diagrams, activity diagrams, and sequence diagrams. Each diagram type is tailored to effectively represent different perspectives of systems —��whether static relationships, interactive behaviors, or dynamic sequences of operations. In the context of analyzing scientific papers, UML’s robust ability to represent complex interactions and logical sequences becomes particularly valuable. Sequence diagrams, one of UML’s behavioral diagram types, are especially suited to illustrating dynamic interactions and the temporal progression of events. They clearly depict how different entities or components communicate and interact over time, making them ideal for modeling logical arguments, evidence flows, and conclusions within academic literature.</p><h3>The main 3 diagrams you need to master</h3><h3>Use Case Diagram</h3><p>Purpose: Captures functional requirements by showing the interactions between external actors (users or systems) and the feature.</p><p>When to use: At the very start of feature design, to align stakeholders on what the feature must do and who will use it.</p><h4>Key elements:</h4><p>Actors (stick figures): represent roles interacting with the system</p><p>Use cases (ovals): represent specific user goals or functions</p><p>System boundary (rectangle): encapsulates the feature’s scope</p><p>Associations (lines): connect actors to the use cases they participate in</p><p>Benefits: Clarifies scope, highlights user needs, uncovers missing requirements.</p><h3>Sequence Diagram</h3><p>Purpose: Models the dynamic interactions and message flows between objects/components over time.</p><p>When to use: Once you know the key actors and use cases; to detail the step-by-step flow for a particular scenario or operation within the feature.</p><h4>Key elements:</h4><p>Lifelines (vertical dashed lines): represent participant objects or components</p><p>Activation bars (thin rectangles on lifelines): indicate when an object is active</p><p>Messages (horizontal arrows): show calls or data transfers, labeled with operation names</p><p>Return messages (dashed arrows): show responses or returned values</p><p>Benefits: Identifies required interfaces, clarifies sequencing and concurrency, surfaces timing or ordering issues early.</p><h3>Class Diagram</h3><p>Purpose: Defines the static structure of the feature by showing classes, their attributes, methods, and relationships.</p><p>When to use: After you’ve specified interactions (via sequence diagrams) and know which objects must exist; to consolidate data structures and API contracts.</p><h4>Key elements:</h4><p>Classes (rectangles divided into three compartments): name, attributes, and operations</p><p>Associations (lines): depict relationships (e.g., one-to-many) between classes</p><p>Inheritance/generalization (lines with hollow triangle): show parent/child hierarchies</p><p>Dependencies (dashed arrows): indicate usage without ownership</p><p>Benefits: Provides a clear blueprint for implementation, aids in code generation or manual coding, ensures a consistent object model.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=711e5510de29" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
        <item>
            <title>Superintelligence Isn’t Imminent — And The Illusion of Thinking.</title>
            <link>https://frankcozzolino.github.io/blog.html?article=superintelligence-isnt-imminent-and-the-illusion-o</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=superintelligence-isnt-imminent-and-the-illusion-o</guid>
            <pubDate>Tue, 17 Jun 2025 08:57:14 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>artificial-intelligence</category>
            <category>ai-research</category>
            <category>ai</category>
            <category>cognitive-science</category>
            <category>machine-learning</category>
            <description>🧠 Superintelligence Isn’t Imminent — And The Illusion of Thinking.Despite all the hype around superintelligent AI systems poised to surpass human capabilities, a fresh wave of research paints a very...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*ZshpwrA1mv_wEd7aAhvlgA.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3>🧠 Superintelligence Isn’t Imminent — And The Illusion of Thinking.</h3><p>Despite all the hype around superintelligent AI systems poised to surpass human capabilities, a fresh wave of research paints a very different — and much more grounded — picture. Apple’s recent study, <em>“The Illusion of Thinking”</em>, alongside contributions from leading researchers and media outlets, calls into question the narrative of imminent Artificial General Intelligence (AGI). What we’re witnessing, it seems, isn’t the rise of a digital Einstein — but something more brittle, narrow, and surprisingly easy to fool.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZshpwrA1mv_wEd7aAhvlgA.png" /></figure><h3>1. Recent Findings Challenge the Hype</h3><p>In <em>“The Illusion of Thinking”</em>, Apple tested leading large language models (LLMs) from OpenAI, Anthropic, and Google on classic reasoning puzzles like Tower of Hanoi and River Crossing. The results were startling: the models, often perceived as near-omniscient, failed logic problems solvable by school children. Worse, as task complexity increased, their ability to complete the problems dropped sharply — sometimes they even stopped trying mid-task.</p><h4>Recent Findings Challenge the AI Hype</h4><p>Apple’s new 2025 study <em>“The Illusion of Thinking”</em> put leading chain-of-thought LLMs to the test with classic logic puzzles. The team evaluated models from OpenAI (o3), Anthropic (Claude 3.7), and Google (Gemini) on tasks like Tower of Hanoi, a river-crossing problem, checkers jumping, and block-stacking. These puzzles were scaled from trivial (e.g. 1-disk Hanoi) to extremely hard (e.g. 20-disk Hanoi requiring over a million moves). In practice, these are problems a schoolchild can solve by following simple rules, so any AI failure is unexpected. Indeed, the AI models <strong>performed poorly even on moderate puzzles</strong>: for example, they “consistently failed at Tower of Hanoi,” scoring under 80% on a 7-disk puzzle and essentially 0% at 8 disks. Similar breakdowns occurred on the river-crossing and block-stacking puzzles, revealing that these high-profile LLMs could not handle basic logical reasoning.</p><h4>Key Findings</h4><ul><li><strong>Complexity regimes:</strong> The Apple paper identifies three regimes. On very easy tasks, <em>standard</em> LLMs (no extra “chain-of-thought”) actually outperformed the chain-of-thought LRMs; on moderately hard tasks, the LRMs had an edge; on very hard tasks, <em>all</em> models collapsed.</li><li><strong>Complete collapse:</strong> The authors observe a “complete accuracy collapse beyond certain complexities”. In other words, once puzzle size passes a threshold, none of the models solved it correctly.</li><li><strong>Specific failures:</strong> In concrete terms, the LLMs <em>“consistently failed”</em> the test puzzles. For example, accuracy plunged below 80% on a 7-disk Tower of Hanoi and to ~0% at 8 disks. They also failed the Blocks World and river-crossing puzzles with the same ease as flipping heads on a coin.</li><li><strong>Solution truncation:</strong> The models often stopped mid-solution once answers got long. In practice, a model might begin listing the moves and then insert “I’ll stop here” when the token budget is reached. (Critics note these token limits can mimic a “collapse.”)</li><li><strong>No explicit algorithms:</strong> Apple found that LRMs “fail to use explicit algorithms and reason inconsistently across puzzles”. In short, the model’s internal “thinking” was erratic rather than systematic.</li></ul><p>These outcomes show that current LLMs’ impressive chain-of-thought outputs can still break down under moderate complexity. Beyond a certain point, giving extra “thinking steps” does <em>not</em> make the models solve problems correctly. In fact, the study concludes that these reasoning models only work up to a limited complexity, after which their performance collapses catastrophically.</p><h4>Implications for AI Understanding</h4><p>Even when LLMs produce elaborate internal “reasoning,” that process may not be logical. Apple’s co-author Iman Mirzadeh emphasizes that even when the model was <em>given</em> the correct solution algorithm, it “still failed” the puzzle — its process “is not logical and intelligent”. In other words, the step-by-step chain-of-thought it outputs can be misleading. As MacDailyNews reports, the study suggests these AI models simply “generate responses that align with pattern-matching” from their training data, rather than deducing new logic. This echoes Gary Marcus’s point that neural nets can only “generalize within a distribution” they have seen and “tend to break down beyond that distribution”. In short, <strong>highly polished AI answers are not proof of genuine understanding</strong>; the models may be drawing on learned patterns instead of applying true reasoning.</p><h4>Critiques and Counterpoints</h4><p>These surprising failures have prompted debate. Open Philanthropy researcher Alex Lawsen argues that the reported “collapse” often reflects evaluation choices, not just model limitations. For example, at the point Apple marked the 8-disk Hanoi as failed, the model was already bumping into its token limit (even printing “I’ll stop here”). Lawsen found that if you instead prompt the model to output a concise program (e.g. a recursive algorithm) rather than listing every move, the models easily solve much larger instances (15-disk Hanoi). He also notes Apple’s river-crossing tests included impossible configurations (no solution), so a correct “no solution” answer was scored as a failure. These points suggest the AI was partly failing due to output-format constraints, not only reasoning deficits. Nonetheless, Lawsen agrees that current LLMs still lack robust, generalizable reasoning: truly proving algorithmic intelligence remains an open challenge. In any case, both the original study and its critiques highlight that how we test “reasoning” matters. Future benchmarks must separate genuine logical skill from practical output limits.</p><p>In summary, the Apple puzzle experiments offer a reality check: even cutting-edge LLMs can falter on deceptively simple tasks, underscoring the gap between <em>appearance</em> and <em>understanding</em>. As Futurism puts it, these models “are no substitute for good, well-specified algorithms,” and their impressive outputs should not be taken as evidence of true intelligence.</p><h3>2. Limits in Logic and Reasoning</h3><p>One key insight from Apple’s research is that current LLMs rely heavily on <strong>pattern recognition</strong>, not true reasoning. They excel when regurgitating information that resembles their training data but falter when faced with novelty. As complexity rises or context shifts, their logical scaffolding crumbles.</p><p>This means we are still far from AIs that can <em>reason</em> through real-world problems in flexible, adaptive ways.</p><h3>3. Superintelligence Trust Outpaces Capabilities</h3><p>Tech leaders like Sam Altman (OpenAI), Demis Hassabis (Google DeepMind), and Dario Amodei (Anthropic) have publicly speculated about the nearness of AGI. But Apple’s findings — and corroborating critiques — suggest that these forecasts are <strong>leaps ahead of actual performance</strong>.</p><p>While marketing claims lean toward inevitability, the models’ analytical capabilities lag significantly behind expectations. Trust in “superintelligence” is growing faster than the tech’s actual reasoning abilities.</p><h3>4. Emergent Tasks ≠ General Reasoning</h3><p>Some AI advocates point to “emergent behavior” as evidence of intelligence — tasks the model seems to solve despite no direct training. Yet, <strong>emergence doesn’t equal generality</strong>.</p><p>These systems often display narrow brilliance that doesn’t transfer outside very specific formats. Being good at solving math benchmarks or chess problems doesn’t mean the model can reason abstractly or solve unfamiliar, open-ended questions.</p><h3>5. Memory Without Comprehension</h3><p>These models are <strong>masters of mimicry</strong>, not meaning. They recall and remix text without true comprehension — an issue compared by some researchers to a “stochastic parrot.” They can ace standardized tests by parroting patterns but lack conceptual frameworks.</p><p>This leads to superficial results: a model that can draft a research summary or pass a quiz may not “understand” anything it says.</p><h3>6. Pattern vs. Reason: Hybrid Intelligence Is a Smarter Goal</h3><p>Emerging academic consensus suggests that scaling current models may <strong>not</strong> bring us closer to genuine intelligence. Instead, <strong>hybrid intelligence</strong> — systems combining AI’s computational power with human intuition and conceptual reasoning — may be the more realistic and safe trajectory.</p><p>Rather than replacing humans, future systems could <strong>collaborate</strong> with us, complementing our strengths rather than mimicking our thinking.</p><h3>7. Superintelligence Raises Complex Control Problems</h3><p>The theoretical risks of superintelligent systems — like alignment failure and existential threats — have been explored by scholars like Nick Bostrom. But Apple’s study reinforces that we’re <strong>nowhere near that threshold</strong>.</p><p>We may one day face such control dilemmas, but right now, a bigger problem is the <strong>overconfidence in AI’s current capabilities</strong> — a misunderstanding that could lead to careless deployment or misplaced trust.</p><h3>8. Policy Implications Demand Realism</h3><p>As governments rush to regulate AI, <strong>misjudging its capabilities could backfire</strong>. Overestimating what these systems can do may lead to inappropriate legal frameworks, poorly allocated funding, or even political fear-mongering.</p><p>Effective policy should be rooted in <strong>what AI actually is</strong>, not in sci-fi predictions. Apple’s research urges a recalibration of the narrative.</p><h3>9. Household Tasks Don’t Equal Awareness</h3><p>Writing a blog post, generating a spreadsheet formula, or replying to emails may seem intelligent — but none of this requires <strong>awareness or understanding</strong>. It’s the <em>illusion</em> of thinking, not thinking itself.</p><p>Today’s systems appear useful not because they grasp meaning, but because they’ve digested billions of examples. That’s not intelligence — it’s compression.</p><h3>10. Hype vs. Reality: Why Scrutiny Matters</h3><p>Calling out AI’s limitations isn’t being cynical — <strong>it’s necessary</strong>. We can’t build responsible systems if we’re blinded by buzzwords. Apple’s work is a reminder: <strong>understanding where we are helps ensure we go further, better, and safer</strong>.</p><h3>Thank you for being a part of the community</h3><p><em>Before you go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writer ️👏<strong>️️</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> | <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a> 🚀</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a> 🧑🏻‍💻</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=69018636646e" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/superintelligence-isnt-imminent-and-the-illusion-of-thinking-69018636646e">🧠 Superintelligence Isn’t Imminent — And The Illusion of Thinking.</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>Vibe Coding: A New Kind of Coding. — Part 1/5</title>
            <link>https://frankcozzolino.github.io/blog.html?article=vibe-coding-a-new-kind-of-coding-part-15</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=vibe-coding-a-new-kind-of-coding-part-15</guid>
            <pubDate>Mon, 16 Jun 2025 10:26:53 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>ai-programming</category>
            <category>vibe-coding</category>
            <category>prompt-engineering</category>
            <category>ai-productivity</category>
            <category>llm-agent</category>
            <description>Vibe Coding: A New Kind of Coding. — Part 1/5Vibe coding is a high-level framework that abstracts the complexity of GPU and parallel programming, making CUDA kernels and thread management accessible...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/0*Yv2vRwl-OX80wLDA" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h3><strong>Vibe Coding: A New Kind of Coding. — Part 1/5</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Yv2vRwl-OX80wLDA" /></figure><blockquote>Vibe coding is a high-level framework that abstracts the complexity of GPU and parallel programming, making CUDA kernels and thread management accessible even to developers without deep specialized knowledge. By auto-optimizing memory transfers and kernel configurations, it delivers both higher performance and reduced energy consumption. Coupled with extensive UML-based design modeling, covering class, sequence, and deployment diagrams, teams can identify security risks early, enforce robust architectures, and minimize implementation flaws. Together, these approaches democratize high-performance computing, lower operational costs, and drive humanity toward faster, greener, and more secure software innovation.</blockquote><p>There’s never much room for feeling in the cold logic of software engineering. Programs either run or they don’t. Brackets match or they don’t. The compiler doesn’t care how inspired you felt at 2 a.m. But in 2025, a different kind of code is emerging — not just from logic gates, but from architectural vision and natural language. It’s structured, holistic, and principled. And it’s called vibe coding.</p><p>While originally coined by Andrej Karpathy in a whimsical tweet, the term has since evolved far beyond its tongue-in-cheek roots. No longer shorthand for improvisational tinkering, vibe coding now represents a movement: a way of programming that empowers developers to zoom out and orchestrate systems, rather than wrestle endlessly with syntax and bugs.</p><p>At its core, vibe coding is about clarity. It leverages AI not to bypass engineering, but to elevate it. By translating architectural plans into executable code, developers reclaim time lost in repetitive problem-solving. Instead of fixing broken imports or hunting for elusive null pointer exceptions, they’re spending that time crafting better user experiences, embedding resilience into their systems, and refining their security posture.</p><p>This is why UML diagrams and detailed technical design have become essential tools in vibe coding workflows. They serve as the blueprint from which all code generation is orchestrated. In the world of vibe coding, a well-structured technical plan isn’t a nice-to-have — it’s the beating heart of the process. These visual schematics help align teams, inform prompts, and ensure consistency across modules, APIs, and data flows. The stronger the architecture, the more precise the AI’s contribution.</p><p>It works through prompt-driven development. The developer writes natural language descriptions of the behavior or architecture they want. AI agents — be it Cursor, Copilot, Claude, or custom tooling — generate code that matches the spec. But the human role doesn’t end there. It’s about guiding, auditing, and integrating that output with foresight and discipline. In short: vibe coding isn’t hands-off — it’s hands-on, but eyes up.</p><p>Vibe coding redefines who the architect is. No longer does architectural thinking reside solely with senior engineers or system designers. With the right AI interfaces, even junior developers or domain experts can outline workflows, enforce constraints, and maintain system-wide integrity. This democratization of high-level design is changing the shape of teams and the nature of software planning.</p><p>Security is no afterthought in this world. One of Vibe Coding’s greatest promises lies in its ability to embed secure defaults. Instead of retrofitting authorization rules or validating inputs after vulnerabilities are discovered, developers can encode policies into their prompts from the outset. “Design the API to reject malformed JWT tokens,” one prompt might say. Or, “Only allow data access if the user’s session is active and within a specified role.” The AI generates these flows in real-time, weaving security into the application’s fabric.</p><p>Critics still point fingers. They argue that AI-assisted development encourages laziness, or that it leads to black-box software, code nobody understands. But practitioners push back. They argue that the most responsible use of vibe coding is deeply intentional. The point isn’t to abandon engineering; it’s to sharpen it. To use AI to remove friction, not thought.</p><p>“I used to spend days debugging cascading failures caused by brittle test setups,” says Gabriel Nassar, a software architect at a cybersecurity firm. “Now, I describe my resilience strategy up front, and the AI handles scaffolding. I still validate and test, but I’m not stuck in the weeds. I’m focused on the mission.”</p><p>This shift is also freeing smaller teams to do more with less. By offloading time-consuming boilerplate tasks to AI, vibe coding gives developers more time to focus on rigorous testing. Small teams that once struggled to cover edge cases can now afford to build comprehensive test suites and validate their systems with greater confidence.</p><p>More importantly, vibe coding is helping redefine where engineering excellence can come from. In regions like Europe and the United States, where labor costs are higher and outsourcing has long been seen as a cost-saving necessity, vibe coding offers a new path. It allows local engineers to compete at scale, delivering sophisticated, high-integrity software systems without bloated headcounts or offshore labor. By automating the mundane, vibe coding lets domestic teams focus on quality, innovation, and architecture.</p><p>The tools themselves are maturing. AI models are learning to explain their logic, suggest tests, and integrate with CI/CD systems that enforce code health. Version-controlled prompt chains — complete with architectural diagrams and security annotations — are replacing scattered README files and spaghetti docstrings.</p><p>And perhaps most importantly, vibe coding is cultivating a new programming literacy. One that prizes systems thinking over brute force. One that empowers teams to move faster, not by skipping steps, but by making every step more intelligent.</p><p>The future won’t be built by AI alone. But it may be shaped by those who know how to harness it—not recklessly, but with intent.</p><p>That’s the real vibe.</p><h3>Thank you for being a part of the community</h3><p><em>Before you go:</em></p><ul><li>Be sure to <strong>clap</strong> and <strong>follow</strong> the writer ️👏<strong>️️</strong></li><li>Follow us: <a href="https://x.com/inPlainEngHQ"><strong>X</strong></a> | <a href="https://www.linkedin.com/company/inplainenglish/"><strong>LinkedIn</strong></a> | <a href="https://www.youtube.com/@InPlainEnglish"><strong>YouTube</strong></a> | <a href="https://newsletter.plainenglish.io/"><strong>Newsletter</strong></a> | <a href="https://open.spotify.com/show/7qxylRWKhvZwMz2WuEoua0"><strong>Podcast</strong></a> | <a href="https://twitch.tv/inplainenglish"><strong>Twitch</strong></a></li><li><a href="https://differ.blog/"><strong>Start your own free AI-powered blog on Differ</strong></a> 🚀</li><li><a href="https://discord.gg/in-plain-english-709094664682340443"><strong>Join our content creators community on Discord</strong></a> 🧑🏻‍💻</li><li>For more content, visit <a href="https://plainenglish.io/"><strong>plainenglish.io</strong></a> + <a href="https://stackademic.com/"><strong>stackademic.com</strong></a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=983bc651fbd7" width="1" height="1" alt=""><hr><p><a href="https://blog.stackademic.com/vibe-coding-a-new-kind-of-coding-part-1-5-983bc651fbd7">Vibe Coding: A New Kind of Coding. — Part 1/5</a> was originally published in <a href="https://blog.stackademic.com">Stackademic</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        
        <item>
            <title>OpenAI’s o3-Pro Outranks PhDs — Now the Experts Are Worried</title>
            <link>https://frankcozzolino.github.io/blog.html?article=openais-o3pro-outranks-phds-now-the-experts-are-wo</link>
            <guid>https://frankcozzolino.github.io/blog.html?article=openais-o3pro-outranks-phds-now-the-experts-are-wo</guid>
            <pubDate>Sat, 14 Jun 2025 13:39:08 GMT</pubDate>
            <dc:creator>Francesco C.</dc:creator>
            <category>data-science</category>
            <category>artificial-intelligence</category>
            <category>technology</category>
            <category>science</category>
            <category>education</category>
            <description>OpenAI’s o3-Pro Outranks PhDs — Now the Experts Are WorriedPeople across industry, academia, and social media have lauded OpenAI’s o3-pro for its exceptional performance on graduate-level science...</description>
            <enclosure url="https://cdn-images-1.medium.com/max/1024/1*PTk3xTRiMztBXmkwMylknQ.png" type="image/jpeg" length="0"/>
            <content:encoded><![CDATA[<h2><strong>OpenAI’s o3-Pro Outranks PhDs — Now the Experts Are Worried</strong></h2><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PTk3xTRiMztBXmkwMylknQ.png" /></figure><p>People across industry, academia, and social media have lauded OpenAI’s o3-pro for its exceptional performance on graduate-level science benchmarks — most notably the GPQA Diamond test — where it not only surpasses human expert baselines but also outperforms rival models like Google’s Gemini 2.5 Pro and Anthropic’s Claude 4 Opus, all while demonstrating deep domain reasoning that reflects PhD-level scientific acumen. . At the same time, academic researchers caution that these benchmark victories, achieved through massive trialling of predefined operations, may not fully capture generalizable understanding, underscoring the need for broader evaluation frameworks and vigilant oversight as o3-pro begins to power real-world research and decision-making workflows. .</p><h3>Benchmark Breakthroughs in PhD-Level Science</h3><p>“On PhD-level science questions on the GPQA Diamond benchmark, [o3-pro] scored 84%, again surpassing its predecessors,” reports Cogni Down Under, reflecting a leap above the 69.7% average achieved by human experts with PhDs on the same dataset. . TechCrunch similarly notes that o3-pro “beats Anthropic’s recently released Claude 4 Opus on GPQA Diamond, a test of PhD-level science knowledge” while also outperforming Google’s Gemini 2.5 Pro on AIME 2024, a rigorous mathematics exam. .</p><h3>Academic Perspectives on Expert-Level Reasoning</h3><p>The GPQA Diamond subset contains 198 multiple-choice questions crafted by domain experts pursuing or holding PhDs in biology, physics, and chemistry, with a random-guess baseline of 25% and a human expert baseline of 69.7% . Yet, Rolf Pfister and Hansueli Jud warn that o3’s record performance “raises the question whether systems based on LLMs demonstrate genuine intelligence,” since the model achieves high scores via extensive brute-force trialling rather than true conceptual understanding. . Similarly, reflective benchmarks have shown that while o3-pro’s “private chain of thought” yields impressive accuracy, longer response times and reliance on predefined operations may limit its adaptability to novel, unstructured scientific challenges. .</p><h3>Expert Reactions and Practical Use Cases</h3><p>Ethan Mollick, a leading AI researcher, shared on LinkedIn:</p><p>“Been playing with o3-pro for a bit. It is quite smart. One problem it solved where every other model has failed is making a word ladder from SPACE to EARTH.”</p><p>David Borish, AI strategist at Trace3, emphasizes that o3-pro’s step-by-step reasoning makes it “particularly effective for complex tasks in mathematics, science, and engineering contexts” . Early adopters in research labs report using o3-pro to draft detailed grant proposals, analyze experimental datasets, and generate hypothesis-driven literature reviews — workflows traditionally reserved for senior PhD candidates. .</p><h3>Case Study: University Exam Performance</h3><p>In a striking demonstration of its foundational capabilities, a recent study found that the predecessor model o3 aced a zero-shot university thermodynamics exam — scoring perfectly and outperforming the top students — highlighting the genuine academic rigor that o3-pro inherits and extends for advanced scientific problem solving. .</p><h3>Balancing Breakthroughs with Oversight</h3><p>Despite widespread praise, critics warn of erratic behavior and “jagged frontier” unpredictability in advanced AI models, noting occasional math errors and overconfidence in unfamiliar domains. . As o3-pro takes center stage in PhD-level workflows, experts agree that robust validation, diversified benchmarks, and human-in-the-loop oversight will be essential to harness its full potential responsibly.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8f8ad4e5483e" width="1" height="1" alt="">]]></content:encoded>
        </item>
        
    </channel>
</rss>